This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: src/*.rs
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
src/
  audio_async.rs
  audio_capture.rs
  audio_seg.rs
  broker.rs
  events.rs
  gemini_client.rs
  gemini_ws_json.rs
  gemini_ws.rs
  gemini.rs
  main.rs
  screen.rs
  ui.rs
  util.rs
  video_capture.rs
  ws_writer.rs

================================================================
Files
================================================================

================
File: src/audio_async.rs
================
//! Async audio capture module using PulseAudio's threaded mainloop
//! 
//! This provides proper async integration with tokio by using PulseAudio's
//! threaded mainloop and callback-based API.

use libpulse_binding as pulse;
use pulse::context::{Context, FlagSet as ContextFlagSet, State as ContextState};
use pulse::mainloop::threaded::Mainloop;
use pulse::proplist::Proplist;
use pulse::sample::{Format, Spec};
use pulse::stream::{FlagSet as StreamFlagSet, State as StreamState, Stream};
use std::cell::RefCell;
use std::error::Error;
use std::ops::Deref;
use std::rc::Rc;
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::Arc;
use std::time::Duration;
use tokio::sync::mpsc;
use tracing::{error, info};

/// Async audio capturer using PulseAudio's threaded mainloop
pub struct AsyncAudioCapturer {
    /// Channel for receiving audio chunks
    rx: mpsc::Receiver<Vec<i16>>,
    /// Shutdown flag
    shutdown: Arc<AtomicBool>,
    /// Handle to the background thread
    _handle: std::thread::JoinHandle<()>,
}

impl AsyncAudioCapturer {
    /// Create a new async audio capturer
    pub fn new(app_name: &str, device_name: Option<&str>) -> Result<Self, Box<dyn Error>> {
        let (tx, rx) = mpsc::channel::<Vec<i16>>(128);
        let shutdown = Arc::new(AtomicBool::new(false));
        let shutdown_clone = shutdown.clone();
        
        let app_name = app_name.to_string();
        let device_name = device_name.map(|s| s.to_string());
        
        // Spawn the audio capture thread (not a tokio task, a real OS thread)
        let handle = std::thread::spawn(move || {
            if let Err(e) = run_audio_capture(app_name, device_name, tx, shutdown_clone) {
                error!("Audio capture error: {}", e);
            }
        });
        
        Ok(Self {
            rx,
            shutdown,
            _handle: handle,
        })
    }
    
    /// Read the next chunk of audio data (20ms worth)
    /// Returns None if the capture has ended
    pub async fn read_chunk(&mut self) -> Option<Vec<i16>> {
        self.rx.recv().await
    }
    
    /// Get the device name being used
    pub fn device_name(&self) -> &str {
        "pulse" // TODO: track actual device name
    }
}

impl Drop for AsyncAudioCapturer {
    fn drop(&mut self) {
        self.shutdown.store(true, Ordering::Relaxed);
        // Thread will exit when it sees shutdown flag
    }
}

/// Run the audio capture in a dedicated thread with PulseAudio's threaded mainloop
fn run_audio_capture(
    app_name: String,
    device_name: Option<String>,
    tx: mpsc::Sender<Vec<i16>>,
    shutdown: Arc<AtomicBool>,
) -> Result<(), Box<dyn Error>> {
    // Create the mainloop
    let mainloop = Rc::new(RefCell::new(
        Mainloop::new().ok_or("Failed to create mainloop")?
    ));
    
    // Create property list for the application
    let mut proplist = Proplist::new().ok_or("Failed to create proplist")?;
    proplist.set_str(pulse::proplist::properties::APPLICATION_NAME, &app_name)
        .map_err(|()| "Failed to set application name")?;
    
    // Create context
    let context = Rc::new(RefCell::new(
        Context::new_with_proplist(
            mainloop.borrow().deref(),
            "AudioContext",
            &proplist
        ).ok_or("Failed to create context")?
    ));
    
    // Set state callback to know when we're connected
    let ml_ref = mainloop.clone();
    let context_ref = context.clone();
    context.borrow_mut().set_state_callback(Some(Box::new(move || {
        let state = unsafe { (*context_ref.as_ptr()).get_state() };
        match state {
            ContextState::Ready => {
                let ml = unsafe { &mut *ml_ref.as_ptr() };
                ml.signal(false);
            }
            ContextState::Failed | ContextState::Terminated => {
                let ml = unsafe { &mut *ml_ref.as_ptr() };
                ml.signal(false);
            }
            _ => {}
        }
    })));
    
    // Connect the context
    mainloop.borrow_mut().lock();
    context.borrow_mut().connect(None, ContextFlagSet::NOFLAGS, None)
        .map_err(|e| format!("Failed to connect context: {:?}", e))?;
    mainloop.borrow_mut().unlock();
    
    // Start the mainloop
    mainloop.borrow_mut().start()
        .map_err(|e| format!("Failed to start mainloop: {:?}", e))?;
    
    // Wait for context to be ready
    mainloop.borrow_mut().lock();
    loop {
        match context.borrow().get_state() {
            ContextState::Ready => break,
            ContextState::Failed | ContextState::Terminated => {
                mainloop.borrow_mut().unlock();
                mainloop.borrow_mut().stop();
                return Err("Context connection failed".into());
            }
            _ => {
                mainloop.borrow_mut().wait();
            }
        }
    }
    mainloop.borrow_mut().unlock();
    
    info!("PulseAudio context connected");
    
    // Create the recording stream - 16kHz mono S16LE
    let spec = Spec {
        format: Format::S16le,
        channels: 1,
        rate: 16000,
    };
    
    let stream = Rc::new(RefCell::new(
        Stream::new(
            &mut context.borrow_mut(),
            "AudioStream",
            &spec,
            None
        ).ok_or("Failed to create stream")?
    ));
    
    // Buffer for accumulating samples
    let buffer = Rc::new(RefCell::new(Vec::<i16>::with_capacity(320)));
    
    // Set up the read callback
    let tx_clone = tx.clone();
    let ml_ref = mainloop.clone();
    let stream_ref = stream.clone();
    let buffer_ref = buffer.clone();
    let shutdown_ref = shutdown.clone();
    
    stream.borrow_mut().set_read_callback(Some(Box::new(move |length| {
        if length == 0 {
            return;
        }
        
        // Check for shutdown
        if shutdown_ref.load(Ordering::Relaxed) {
            unsafe {
                let ml = &mut *ml_ref.as_ptr();
                ml.stop();
            }
            return;
        }
        
        // Peek at the data
        let peek_result = unsafe {
            let stream = &mut *stream_ref.as_ptr();
            stream.peek()
        };
        
        match peek_result {
            Ok(pulse::stream::PeekResult::Data(data)) => {
                if !data.is_empty() {
                    // Convert bytes to i16 samples
                    let samples: Vec<i16> = data.chunks_exact(2)
                        .map(|chunk| i16::from_le_bytes([chunk[0], chunk[1]]))
                        .collect();
                    
                    // Accumulate in buffer
                    unsafe {
                        let buffer = &mut *buffer_ref.as_ptr();
                        buffer.extend_from_slice(&samples);
                        
                        // Send complete 20ms chunks (320 samples)
                        while buffer.len() >= 320 {
                            let chunk: Vec<i16> = buffer.drain(..320).collect();
                            // Use blocking send since we're in a thread
                            if tx_clone.blocking_send(chunk).is_err() {
                                // Receiver dropped, initiate shutdown
                                let ml = &mut *ml_ref.as_ptr();
                                ml.stop();
                                return;
                            }
                        }
                    }
                    
                    // Discard the data from the stream
                    unsafe {
                        let stream = &mut *stream_ref.as_ptr();
                        let _ = stream.discard();
                    }
                }
            }
            Ok(pulse::stream::PeekResult::Empty) => {
                // No data available
            }
            Ok(pulse::stream::PeekResult::Hole(_)) => {
                // There's a hole in the buffer, skip it
                unsafe {
                    let stream = &mut *stream_ref.as_ptr();
                    let _ = stream.discard();
                }
            }
            Err(e) => {
                error!("Failed to peek stream data: {:?}", e);
            }
        }
    })));
    
    // Set stream state callback
    let ml_ref = mainloop.clone();
    let stream_ref = stream.clone();
    stream.borrow_mut().set_state_callback(Some(Box::new(move || {
        let state = unsafe {
            let stream = &*stream_ref.as_ptr();
            stream.get_state()
        };
        match state {
            StreamState::Ready => {
                info!("Stream ready");
                unsafe {
                    let ml = &mut *ml_ref.as_ptr();
                    ml.signal(false);
                }
            }
            StreamState::Failed | StreamState::Terminated => {
                error!("Stream failed/terminated");
                unsafe {
                    let ml = &mut *ml_ref.as_ptr();
                    ml.signal(false);
                }
            }
            _ => {}
        }
    })));
    
    // Set buffer attributes for low latency
    let buffer_attr = pulse::def::BufferAttr {
        maxlength: 16000, // 1 second max
        tlength: std::u32::MAX,
        prebuf: std::u32::MAX,
        minreq: std::u32::MAX,
        fragsize: 640, // 20ms chunks (320 samples * 2 bytes)
    };
    
    // Connect the stream for recording
    mainloop.borrow_mut().lock();
    stream.borrow_mut().connect_record(
        device_name.as_deref(),
        Some(&buffer_attr),
        StreamFlagSet::ADJUST_LATENCY | StreamFlagSet::AUTO_TIMING_UPDATE
    ).map_err(|e| format!("Failed to connect recording stream: {:?}", e))?;
    mainloop.borrow_mut().unlock();
    
    // Wait for stream to be ready
    mainloop.borrow_mut().lock();
    loop {
        match stream.borrow().get_state() {
            StreamState::Ready => break,
            StreamState::Failed | StreamState::Terminated => {
                mainloop.borrow_mut().unlock();
                mainloop.borrow_mut().stop();
                return Err("Stream connection failed".into());
            }
            _ => {
                mainloop.borrow_mut().wait();
            }
        }
    }
    mainloop.borrow_mut().unlock();
    
    info!("Audio stream ready, starting capture");
    
    // The threaded mainloop runs in its own thread
    // We just need to wait for shutdown signal
    loop {
        std::thread::sleep(Duration::from_millis(100));
        if shutdown.load(Ordering::Relaxed) {
            break;
        }
    }
    
    // Cleanup
    mainloop.borrow_mut().lock();
    stream.borrow_mut().disconnect().ok();
    context.borrow_mut().disconnect();
    mainloop.borrow_mut().unlock();
    mainloop.borrow_mut().stop();
    
    Ok(())
}

================
File: src/audio_capture.rs
================
use crate::events::InEvent;
use anyhow::Result;
use libpulse_binding::sample::{Format, Spec};
use libpulse_simple_binding::Simple;
use libpulse_binding::stream::Direction;
use tokio::sync::mpsc::UnboundedSender;
use tokio::time::{interval, Duration};

pub fn spawn(tx: UnboundedSender<InEvent>) -> Result<()> {
    tokio::spawn(async move {
        if let Err(e) = capture_loop(tx).await {
            eprintln!("Audio capture error: {}", e);
        }
    });
    Ok(())
}

pub fn spawn_with_dual_output(tx1: UnboundedSender<InEvent>, tx2: UnboundedSender<InEvent>) -> Result<()> {
    tokio::spawn(async move {
        if let Err(e) = capture_loop_dual(tx1, tx2).await {
            eprintln!("Audio capture error: {}", e);
        }
    });
    Ok(())
}

async fn capture_loop(tx: UnboundedSender<InEvent>) -> Result<()> {
    let spec = Spec {
        format: Format::S16le,
        channels: 1,
        rate: 16000,
    };

    let simple = Simple::new(
        None,
        "rholive",
        Direction::Record,
        None,
        "record",
        &spec,
        None,
        None,
    )?;

    let mut ticker = interval(Duration::from_millis(20));
    let samples_per_chunk = 320;

    loop {
        ticker.tick().await;
        
        let mut buffer = vec![0i16; samples_per_chunk];
        let bytes = unsafe {
            std::slice::from_raw_parts_mut(
                buffer.as_mut_ptr() as *mut u8,
                buffer.len() * 2,
            )
        };

        match simple.read(bytes) {
            Ok(_) => {
                if tx.send(InEvent::AudioChunk(buffer)).is_err() {
                    break;
                }
            }
            Err(e) => {
                eprintln!("Audio read error: {}", e);
                continue;
            }
        }
    }

    Ok(())
}

async fn capture_loop_dual(tx1: UnboundedSender<InEvent>, tx2: UnboundedSender<InEvent>) -> Result<()> {
    let spec = Spec {
        format: Format::S16le,
        channels: 1,
        rate: 16000,
    };

    let simple = Simple::new(
        None,
        "rholive",
        Direction::Record,
        None,
        "record",
        &spec,
        None,
        None,
    )?;

    let mut ticker = interval(Duration::from_millis(20));
    let samples_per_chunk = 320;

    loop {
        ticker.tick().await;
        
        let mut buffer = vec![0i16; samples_per_chunk];
        let bytes = unsafe {
            std::slice::from_raw_parts_mut(
                buffer.as_mut_ptr() as *mut u8,
                buffer.len() * 2,
            )
        };

        match simple.read(bytes) {
            Ok(_) => {
                let event = InEvent::AudioChunk(buffer);
                if tx1.send(event.clone()).is_err() || tx2.send(event).is_err() {
                    break;
                }
            }
            Err(e) => {
                eprintln!("Audio read error: {}", e);
                continue;
            }
        }
    }

    Ok(())
}

================
File: src/audio_seg.rs
================
//! Real-time audio segmentation v2
//!
//! A redesigned architecture that addresses the fundamental timing issues in v1:
//! - Lock-free ring buffer for audio storage
//! - Decoupled VAD, ASR, and boundary decision pipelines
//! - Bounded worst-case latencies with skip-not-wait back-pressure
//! - Tri-stable state machine: Idle → Recording → Committing → Recording

use std::collections::{BTreeMap, VecDeque};
use std::ops::Range;
use std::sync::atomic::{AtomicBool, AtomicU64, AtomicUsize, Ordering};
use std::sync::{mpsc, Arc};
use std::time::{Duration, Instant};
use tracing::{debug, error, warn};
use webrtc_vad::{SampleRate, Vad, VadMode};
use whisper_rs::{FullParams, SamplingStrategy, WhisperContext, WhisperContextParameters};
use crate::events::{TurnInput, Outgoing};

/// Reason why a segment was closed
#[derive(Debug, Clone, PartialEq)]
pub enum CloseReason {
    /// Closed due to silence
    Silence,
    /// Closed due to maximum length
    MaxLength,
    /// Closed due to ASR clause detection
    AsrClause,
}

/// A completed audio segment
#[derive(Debug, Clone)]
pub struct SegmentedTurn {
    pub id: u64,
    pub audio: Vec<i16>,
    pub close_reason: CloseReason,
    pub text: Option<String>,
}

/// Configuration for the segmenter
#[derive(Debug, Clone)]
pub struct SegConfig {
    /// Number of voiced frames to open a segment (4 frames ≈ 80ms)
    pub open_voiced_frames: usize,
    /// Silence duration to automatically close a segment (ms)
    pub close_silence_ms: u64,
    /// Maximum duration of a turn (ms)
    pub max_turn_ms: u64,
    /// Minimum number of tokens for a valid clause
    pub min_clause_tokens: usize,
    /// Interval between ASR polls during a turn (ms)
    pub asr_poll_ms: u64,
    /// Ring buffer capacity in samples (default: 20 seconds at 16kHz)
    pub ring_capacity: usize,
    /// ASR worker pool size
    pub asr_pool_size: usize,
    /// Maximum time to wait for ASR result before emitting without transcript
    pub asr_timeout_ms: u64,
}

impl Default for SegConfig {
    fn default() -> Self {
        Self {
            open_voiced_frames: 6,      // 120ms
            close_silence_ms: 300,      // 300ms
            max_turn_ms: 5000,          // 5 seconds for responsive interaction
            min_clause_tokens: 4,       // 4 tokens minimum
            asr_poll_ms: 250,           // 250ms ASR polling
            ring_capacity: 320_000,     // 20 seconds at 16kHz
            asr_pool_size: 2,           // 2 worker threads
            asr_timeout_ms: 2000,       // 2 second timeout
        }
    }
}

/// Lock-free ring buffer for audio samples
pub struct AudioRingBuffer {
    buffer: Vec<i16>,
    capacity: usize,
    write_pos: AtomicUsize,
    /// Global sample index (monotonically increasing)
    global_idx: AtomicUsize,
}

impl AudioRingBuffer {
    pub fn new(capacity: usize) -> Self {
        Self {
            buffer: vec![0; capacity],
            capacity,
            write_pos: AtomicUsize::new(0),
            global_idx: AtomicUsize::new(0),
        }
    }

    /// Push a frame of samples, returns the global index of the first sample
    pub fn push_frame(&self, samples: &[i16]) -> usize {
        let start_global_idx = self.global_idx.load(Ordering::Acquire);
        let write_pos = self.write_pos.load(Ordering::Acquire);
        
        // Copy samples into ring buffer
        for (i, &sample) in samples.iter().enumerate() {
            let pos = (write_pos + i) % self.capacity;
            // SAFETY: We're the only writer, and pos is always in bounds
            unsafe {
                // Use atomic store - this is safe because we're the only writer
                std::ptr::write_volatile(self.buffer.as_ptr().add(pos) as *mut i16, sample);
            }
        }
        
        // Update positions atomically
        let new_write_pos = (write_pos + samples.len()) % self.capacity;
        self.write_pos.store(new_write_pos, Ordering::Release);
        self.global_idx.store(start_global_idx + samples.len(), Ordering::Release);
        
        start_global_idx
    }

    /// Get a snapshot of samples for the given global index range
    /// Returns None if the range is no longer available in the ring
    pub fn get_range(&self, range: Range<usize>) -> Option<Vec<i16>> {
        let current_global = self.global_idx.load(Ordering::Acquire);
        let available_start = current_global.saturating_sub(self.capacity);
        
        // Check if range is still available
        if range.start < available_start || range.end > current_global {
            return None;
        }
        
        let write_pos = self.write_pos.load(Ordering::Acquire);
        let mut result = Vec::with_capacity(range.len());
        
        for global_idx in range {
            let ring_pos = (write_pos + self.capacity - (current_global - global_idx)) % self.capacity;
            // SAFETY: ring_pos is always in bounds due to the availability check above
            unsafe {
                result.push(*self.buffer.get_unchecked(ring_pos));
            }
        }
        
        Some(result)
    }

    /// Get current global index
    pub fn current_global_idx(&self) -> usize {
        self.global_idx.load(Ordering::Acquire)
    }
}

/// Metadata for a 20ms frame
#[derive(Debug, Clone)]
pub struct FrameMeta {
    pub timestamp: Instant,
    pub start_idx: usize,  // Global index in ring
    pub voiced: bool,
}

/// ASR proposal for clause boundary
#[derive(Debug, Clone)]
pub struct AsrProposal {
    pub clause_end_idx: usize,  // Global index
    pub text: String,
    pub confidence: f32,
}

/// Boundary detection event
#[derive(Debug, Clone)]
pub enum BoundaryEvent {
    SilenceClose(usize, usize),        // start_idx, end_idx
    MaxLenClose(usize, usize),         // start_idx, end_idx
    AsrClose(usize, usize, String),    // start_idx, end_idx, text
}

/// A committed segment waiting for emission
#[derive(Debug)]
pub struct SegmentCommit {
    pub id: u64,
    pub range: Range<usize>,
    pub reason: CloseReason,
    pub text: Option<String>,
    pub timestamp: Instant,
}

/// Frame classifier that runs VAD on incoming audio
pub struct FrameClassifier {
    vad: Vad,
    frame_queue: mpsc::Sender<FrameMeta>,
}

impl FrameClassifier {
    pub fn new() -> Result<(Self, mpsc::Receiver<FrameMeta>), Box<dyn std::error::Error>> {
        let vad = Vad::new_with_rate_and_mode(SampleRate::Rate16kHz, VadMode::VeryAggressive);
        let (tx, rx) = mpsc::channel();
        
        Ok((Self {
            vad,
            frame_queue: tx,
        }, rx))
    }

    /// Classify a 20ms frame (320 samples)
    pub fn classify_frame(&mut self, samples: &[i16], global_idx: usize, timestamp: Instant) -> Result<(), Box<dyn std::error::Error>> {
        if samples.len() != 320 {
            return Err(format!("Expected 320 samples for 20ms frame, got {}", samples.len()).into());
        }

        let voiced = self.vad.is_voice_segment(samples).map_err(|_| "VAD error")?;
        
        let frame_meta = FrameMeta {
            timestamp,
            start_idx: global_idx,
            voiced,
        };

        if let Err(_) = self.frame_queue.send(frame_meta) {
            warn!("Frame queue full, dropping frame");
        }

        Ok(())
    }
}

/// States for the boundary detection FSM
#[derive(Debug, Clone, PartialEq)]
pub enum BoundaryState {
    Idle,
    Recording {
        seg_start_idx: usize,
        last_voice_idx: usize,
        started_at: Instant,
    },
    Committing {
        seg_start_idx: usize,
        last_voice_idx: usize,
        started_at: Instant,
    },
}

/// Finite state machine for boundary detection
pub struct BoundaryFSM {
    config: SegConfig,
    state: BoundaryState,
    voiced_score: f32,
    next_seg_id: u64,
    boundary_events: mpsc::Sender<BoundaryEvent>,
    asr_proposals: mpsc::Receiver<AsrProposal>,
}

impl BoundaryFSM {
    pub fn new(
        config: SegConfig,
        asr_proposals: mpsc::Receiver<AsrProposal>,
    ) -> (Self, mpsc::Receiver<BoundaryEvent>) {
        let (boundary_tx, boundary_rx) = mpsc::channel();
        
        (Self {
            config,
            state: BoundaryState::Idle,
            voiced_score: 0.0,
            next_seg_id: 1,
            boundary_events: boundary_tx,
            asr_proposals,
        }, boundary_rx)
    }

    /// Process a frame and potentially emit boundary events
    pub fn process_frame(&mut self, frame: &FrameMeta, current_global_idx: usize) {
        // Update voiced score with decay
        self.voiced_score = self.voiced_score * 0.75 + if frame.voiced { 1.0 } else { 0.0 };
        
        // Check for ASR proposals
        while let Ok(proposal) = self.asr_proposals.try_recv() {
            self.handle_asr_proposal(proposal, current_global_idx);
        }
        
        let now = Instant::now();
        
        match &self.state {
            BoundaryState::Idle => {
                // Check for opening condition
                if self.voiced_score >= 3.0 { // ~60ms of speech
                    let seg_start_idx = frame.start_idx.saturating_sub(8000); // 500ms pre-roll
                    debug!("Opening segment {} at idx {}", self.next_seg_id, seg_start_idx);
                    
                    self.state = BoundaryState::Recording {
                        seg_start_idx,
                        last_voice_idx: frame.start_idx,
                        started_at: now,
                    };
                }
            }
            
            BoundaryState::Recording { seg_start_idx, last_voice_idx, started_at } => {
                let mut new_last_voice = *last_voice_idx;
                if frame.voiced {
                    new_last_voice = frame.start_idx;
                }
                
                // Check closing conditions
                let elapsed = now.duration_since(*started_at);
                let silence_samples = current_global_idx.saturating_sub(new_last_voice);
                let silence_ms = (silence_samples * 1000) / 16000; // Convert to ms
                
                let close_event = if elapsed.as_millis() as u64 >= self.config.max_turn_ms {
                    Some(BoundaryEvent::MaxLenClose(*seg_start_idx, current_global_idx))
                } else if silence_ms >= self.config.close_silence_ms as usize {
                    Some(BoundaryEvent::SilenceClose(*seg_start_idx, current_global_idx))
                } else {
                    None
                };
                
                if let Some(event) = close_event {
                    debug!("Closing segment {} due to {:?}", self.next_seg_id, event);
                    let _ = self.boundary_events.send(event);
                    self.next_seg_id += 1;
                    self.state = BoundaryState::Idle;
                    self.voiced_score = 0.0;
                } else {
                    // Update state with new voice position
                    self.state = BoundaryState::Recording {
                        seg_start_idx: *seg_start_idx,
                        last_voice_idx: new_last_voice,
                        started_at: *started_at,
                    };
                }
            }
            
            BoundaryState::Committing { .. } => {
                // In committing state, check if we should start a new segment
                if self.voiced_score >= 3.0 {
                    let seg_start_idx = frame.start_idx.saturating_sub(1600); // 100ms pre-roll
                    self.state = BoundaryState::Recording {
                        seg_start_idx,
                        last_voice_idx: frame.start_idx,
                        started_at: now,
                    };
                }
            }
        }
    }

    fn handle_asr_proposal(&mut self, proposal: AsrProposal, current_global_idx: usize) {
        // Only handle ASR proposals if we're in Recording state
        if let BoundaryState::Recording { seg_start_idx, .. } = &self.state {
            // Validate that the proposal is for current segment and represents a valid clause
            if proposal.clause_end_idx > *seg_start_idx && 
               proposal.clause_end_idx < current_global_idx &&
               self.is_valid_clause(&proposal.text) {
                
                debug!("ASR clause detected: '{}' ending at {}", proposal.text, proposal.clause_end_idx);
                let event = BoundaryEvent::AsrClose(*seg_start_idx, proposal.clause_end_idx, proposal.text);
                let _ = self.boundary_events.send(event);
                self.next_seg_id += 1;
                
                // Transition to committing state to handle remaining audio
                self.state = BoundaryState::Committing {
                    seg_start_idx: proposal.clause_end_idx,
                    last_voice_idx: proposal.clause_end_idx,
                    started_at: Instant::now(),
                };
            }
        }
    }

    fn is_valid_clause(&self, text: &str) -> bool {
        let t = text.trim();
        if t.is_empty() {
            return false;
        }

        // Always accept explicit sentence enders
        if t.ends_with(['.', '?', '!', ';']) {
            return true;
        }

        // Token threshold (≈ words)
        let tokens = t.split_whitespace().count();
        if tokens >= self.config.min_clause_tokens {
            return true;
        }

        // Speech disfluency markers
        matches!(t.chars().last().unwrap_or(' '), ',' | '-')
            || t.ends_with(" and")
            || t.ends_with(" but")
            || t.contains(" because ")
    }

    pub fn get_current_segment_range(&self) -> Option<Range<usize>> {
        match &self.state {
            BoundaryState::Recording { seg_start_idx, .. } => Some(*seg_start_idx..usize::MAX),
            BoundaryState::Committing { seg_start_idx, .. } => Some(*seg_start_idx..usize::MAX),
            BoundaryState::Idle => None,
        }
    }
    
    pub fn get_state(&self) -> &BoundaryState {
        &self.state
    }
}

/// Request to ASR worker pool
#[derive(Debug)]
struct AsrRequest {
    id: u64,
    audio: Vec<i16>,
    global_range: Range<usize>,
}

/// ASR worker pool for semantic analysis
pub struct AsrWorkerPool {
    workers: Vec<std::thread::JoinHandle<()>>,
    request_tx: mpsc::Sender<AsrRequest>,
    shutdown: Arc<AtomicBool>,
}

impl AsrWorkerPool {
    pub fn new(
        config: &SegConfig,
        whisper_model: Option<&std::path::Path>,
        proposal_tx: mpsc::Sender<AsrProposal>,
    ) -> Result<Self, Box<dyn std::error::Error>> {
        let (request_tx, request_rx) = mpsc::channel();
        let shutdown = Arc::new(AtomicBool::new(false));
        
        let mut workers = Vec::new();
        
        if let Some(model_path) = whisper_model {
            let ctx = Arc::new(WhisperContext::new_with_params(
                model_path.to_str().unwrap(),
                WhisperContextParameters::default(),
            )?);
            
            // Use shared receiver for multiple workers
            let request_rx = Arc::new(std::sync::Mutex::new(request_rx));
            
            for worker_id in 0..config.asr_pool_size {
                let ctx_clone = ctx.clone();
                let request_rx_clone = request_rx.clone();
                let proposal_tx_clone = proposal_tx.clone();
                let shutdown_clone = shutdown.clone();
                let min_tokens = config.min_clause_tokens;
                
                let worker = std::thread::spawn(move || {
                    asr_worker_shared(worker_id, request_rx_clone, proposal_tx_clone, ctx_clone, shutdown_clone, min_tokens);
                });
                
                workers.push(worker);
            }
        }
        
        Ok(Self {
            workers,
            request_tx,
            shutdown,
        })
    }

    /// Submit audio for ASR processing (non-blocking)
    pub fn submit(&self, id: u64, audio: Vec<i16>, global_range: Range<usize>) -> bool {
        let request = AsrRequest { id, audio, global_range };
        self.request_tx.send(request).is_ok()
    }

    pub fn shutdown(&self) {
        self.shutdown.store(true, Ordering::Release);
    }
}

impl Drop for AsrWorkerPool {
    fn drop(&mut self) {
        self.shutdown();
        // Don't wait for workers to finish - they'll detect shutdown and exit
    }
}

/// ASR worker function with shared receiver
fn asr_worker_shared(
    worker_id: usize,
    request_rx: Arc<std::sync::Mutex<mpsc::Receiver<AsrRequest>>>,
    proposal_tx: mpsc::Sender<AsrProposal>,
    ctx: Arc<WhisperContext>,
    shutdown: Arc<AtomicBool>,
    min_tokens: usize,
) {
    debug!("ASR worker {} started", worker_id);
    
    while !shutdown.load(Ordering::Acquire) {
        // Wait for request with timeout
        let request = match request_rx.lock().unwrap().recv_timeout(Duration::from_millis(100)) {
            Ok(req) => req,
            Err(mpsc::RecvTimeoutError::Timeout) => continue,
            Err(mpsc::RecvTimeoutError::Disconnected) => break,
        };
        
        debug!("Worker {} processing {} samples", worker_id, request.audio.len());
        
        // Create Whisper state
        let mut state = match ctx.create_state() {
            Ok(state) => state,
            Err(e) => {
                error!("Worker {} failed to create Whisper state: {}", worker_id, e);
                continue;
            }
        };
        
        // Set up parameters
        let mut params = FullParams::new(SamplingStrategy::Greedy { best_of: 1 });
        params.set_language(Some("en"));
        params.set_print_special(false);
        params.set_print_progress(false);
        params.set_print_realtime(false);
        params.set_print_timestamps(false);
        params.set_token_timestamps(true);
        
        // Convert to f32 and ensure minimum length
        let mut audio: Vec<f32> = request.audio.iter().map(|&s| s as f32 / 32768.0).collect();
        if audio.len() < 16080 {
            audio.resize(16080, 0.0);
        }
        
        // Run inference
        if let Err(e) = state.full(params, &audio) {
            error!("Worker {} inference failed: {}", worker_id, e);
            continue;
        }
        
        // Extract clause boundaries
        if let Some(proposal) = extract_clause_boundary(&state, &request.global_range, min_tokens) {
            if let Err(_) = proposal_tx.send(proposal) {
                warn!("Worker {} proposal queue full", worker_id);
            }
        }
    }
    
    debug!("ASR worker {} shutting down", worker_id);
}

/// Extract the first valid clause boundary from Whisper results
fn extract_clause_boundary(
    state: &whisper_rs::WhisperState,
    global_range: &Range<usize>,
    min_tokens: usize,
) -> Option<AsrProposal> {
    let n_segments = state.full_n_segments().unwrap_or(0);
    if n_segments == 0 {
        return None;
    }
    
    let full_text = state.full_get_segment_text(0).unwrap_or_default().to_string();
    if full_text.trim().is_empty() {
        return None;
    }
    
    // Find first valid clause boundary
    if let Ok(n_tokens) = state.full_n_tokens(0) {
        let mut current_text = String::new();
        
        for i in 0..n_tokens {
            if let (Ok(token_text), Ok(token_data)) = 
                (state.full_get_token_text(0, i), state.full_get_token_data(0, i)) {
                
                if !token_text.starts_with('[') {
                    current_text.push_str(&token_text);
                }
                
                if is_valid_clause_simple(&current_text, min_tokens) {
                    // Convert centiseconds to global sample index
                    let time_offset_samples = (token_data.t1 as f32 * 0.01 * 16000.0) as usize;
                    let clause_end_idx = global_range.start + time_offset_samples;
                    
                    if clause_end_idx < global_range.end {
                        return Some(AsrProposal {
                            clause_end_idx,
                            text: current_text.trim().to_string(),
                            confidence: 1.0, // TODO: extract actual confidence
                        });
                    }
                }
            }
        }
    }
    
    None
}

/// Simple clause validation (reused from original)
fn is_valid_clause_simple(text: &str, min_tokens: usize) -> bool {
    let t = text.trim();
    if t.is_empty() {
        return false;
    }

    // Always accept explicit sentence enders
    if t.ends_with(['.', '?', '!', ';']) {
        return true;
    }

    // Token threshold
    let tokens = t.split_whitespace().count();
    if tokens >= min_tokens {
        return true;
    }

    false

    // // Disfluencies
    // matches!(t.chars().last().unwrap_or(' '), ',' | '-')
    //     || t.ends_with(" and")
    //     || t.ends_with(" but")
    //     || t.contains(" because ")
}

/// Segment emitter that converts commits to final segments
pub struct SegmentEmitter {
    config: SegConfig,
    ring_buffer: Arc<AudioRingBuffer>,
    pending_commits: BTreeMap<u64, SegmentCommit>,
    next_emit_id: u64,
    output_queue: VecDeque<SegmentedTurn>,
}

impl SegmentEmitter {
    pub fn new(config: SegConfig, ring_buffer: Arc<AudioRingBuffer>) -> Self {
        Self {
            config,
            ring_buffer,
            pending_commits: BTreeMap::new(),
            next_emit_id: 1,
            output_queue: VecDeque::new(),
        }
    }

    /// Process a boundary event and create a commit
    pub fn process_boundary_event(&mut self, event: BoundaryEvent, seg_id: u64) {
        let (start_idx, end_idx, reason, text) = match event {
            BoundaryEvent::SilenceClose(start_idx, end_idx) => (start_idx, end_idx, CloseReason::Silence, None),
            BoundaryEvent::MaxLenClose(start_idx, end_idx) => (start_idx, end_idx, CloseReason::MaxLength, None),
            BoundaryEvent::AsrClose(start_idx, end_idx, text) => (start_idx, end_idx, CloseReason::AsrClause, Some(text)),
        };

        let commit = SegmentCommit {
            id: seg_id,
            range: start_idx..end_idx,
            reason,
            text,
            timestamp: Instant::now(),
        };

        self.pending_commits.insert(seg_id, commit);
        self.try_emit_ready_segments();
    }

    /// Add transcript to existing commit
    pub fn add_transcript(&mut self, seg_id: u64, text: String) {
        if let Some(commit) = self.pending_commits.get_mut(&seg_id) {
            if commit.text.is_none() {
                commit.text = Some(text);
                self.try_emit_ready_segments();
            }
        }
    }

    /// Try to emit segments that are ready
    fn try_emit_ready_segments(&mut self) {
        while let Some(commit) = self.pending_commits.get(&self.next_emit_id) {
            // Check if we should wait for transcript
            let should_wait = commit.text.is_none() && 
                commit.reason != CloseReason::AsrClause &&
                commit.timestamp.elapsed().as_millis() < self.config.asr_timeout_ms as u128;

            if should_wait {
                break;
            }

            // Remove from pending and convert to segment
            let commit = self.pending_commits.remove(&self.next_emit_id).unwrap();
            
            if let Some(pcm) = self.ring_buffer.get_range(commit.range) {
                let pcm_len = pcm.len();
                let segment = SegmentedTurn {
                    id: self.next_emit_id,
                    audio: pcm,
                    close_reason: commit.reason,
                    text: commit.text,
                };
                
                self.output_queue.push_back(segment);
                debug!("Emitted segment {} with {} samples", self.next_emit_id, pcm_len);
            } else {
                warn!("Failed to get audio for segment {} - range no longer available", self.next_emit_id);
            }
            
            self.next_emit_id += 1;
        }
    }

    /// Get next ready segment
    pub fn pop_segment(&mut self) -> Option<SegmentedTurn> {
        self.try_emit_ready_segments();
        self.output_queue.pop_front()
    }
}

/// Main v2 audio segmenter
pub struct AudioSegmenter {
    config: SegConfig,
    ring_buffer: Arc<AudioRingBuffer>,
    frame_classifier: FrameClassifier,
    frame_receiver: mpsc::Receiver<FrameMeta>,
    boundary_fsm: BoundaryFSM,
    boundary_receiver: mpsc::Receiver<BoundaryEvent>,
    asr_pool: AsrWorkerPool,
    emitter: SegmentEmitter,
    last_asr_poll: Instant,
    next_asr_id: u64,
    /// Track the last index submitted to ASR to avoid duplicate processing
    last_asr_submit_idx: Option<usize>,
    /// Track the previous FSM state to detect transitions
    prev_fsm_state: Option<BoundaryState>,
    /// Sender for streaming events
    streaming_tx: Option<mpsc::Sender<TurnInput>>,
    /// Sender for outgoing websocket messages
    outgoing_tx: Option<mpsc::Sender<Outgoing>>,
    /// Global turn ID generator (shared across all producers)
    turn_id_generator: Arc<AtomicU64>,
    /// Current turn ID for this segmenter
    current_turn_id: Option<u64>,
}

impl AudioSegmenter {
    pub fn new(
        config: SegConfig,
        whisper_model: Option<&std::path::Path>,
    ) -> Result<Self, Box<dyn std::error::Error>> {
        let ring_buffer = Arc::new(AudioRingBuffer::new(config.ring_capacity));
        
        let (frame_classifier, frame_receiver) = FrameClassifier::new()?;
        
        // Create proposal channel for ASR → FSM communication
        let (asr_proposal_tx, asr_proposal_rx) = mpsc::channel();
        let asr_pool = AsrWorkerPool::new(&config, whisper_model, asr_proposal_tx)?;
        let (boundary_fsm, boundary_receiver) = BoundaryFSM::new(config.clone(), asr_proposal_rx);
        
        let emitter = SegmentEmitter::new(config.clone(), ring_buffer.clone());
        
        Ok(Self {
            config,
            ring_buffer,
            frame_classifier,
            frame_receiver,
            boundary_fsm,
            boundary_receiver,
            asr_pool,
            emitter,
            last_asr_poll: Instant::now(),
            next_asr_id: 1,
            last_asr_submit_idx: None,
            prev_fsm_state: None,
            streaming_tx: None,
            outgoing_tx: None,
            turn_id_generator: Arc::new(AtomicU64::new(0)),
            current_turn_id: None,
        })
    }

    /// Set the streaming event sender
    pub fn set_streaming_sender(&mut self, tx: mpsc::Sender<TurnInput>) {
        self.streaming_tx = Some(tx);
    }
    
    /// Set the outgoing websocket message sender and turn ID generator
    pub fn set_outgoing_sender(&mut self, tx: mpsc::Sender<Outgoing>, turn_id_gen: Arc<AtomicU64>) {
        self.outgoing_tx = Some(tx);
        self.turn_id_generator = turn_id_gen;
    }

    /// Process a 20ms chunk (320 samples at 16kHz)
    pub fn push_chunk(&mut self, chunk: &[i16]) -> Option<SegmentedTurn> {
        if chunk.len() != 320 {
            warn!("Expected 320 samples, got {}", chunk.len());
            return None;
        }

        let timestamp = Instant::now();
        let chunk_start_idx = self.ring_buffer.push_frame(chunk);
        
        // Store current FSM state before processing
        let prev_state = self.prev_fsm_state.clone();
        
        // Process the 20ms frame directly for VAD
        let _ = self.frame_classifier.classify_frame(chunk, chunk_start_idx, timestamp);
        
        // Process frame events
        while let Ok(frame_meta) = self.frame_receiver.try_recv() {
            let current_global_idx = self.ring_buffer.current_global_idx();
            self.boundary_fsm.process_frame(&frame_meta, current_global_idx);
        }
        
        // Check for state transitions and emit outgoing events
        let current_state = self.boundary_fsm.get_state();
        
        // Use new outgoing channel if available, otherwise fall back to streaming
        if let Some(ref tx) = self.outgoing_tx {
            // Check if we just opened a segment (Idle -> Recording)
            if matches!(prev_state, Some(BoundaryState::Idle) | None) && 
               matches!(current_state, BoundaryState::Recording { .. }) {
                // Generate new turn ID
                let turn_id = self.turn_id_generator.fetch_add(1, Ordering::SeqCst);
                self.current_turn_id = Some(turn_id);
                let _ = tx.send(Outgoing::ActivityStart(turn_id));
            }
            
            // Always send the audio chunk if we're recording
            if let Some(turn_id) = self.current_turn_id {
                if matches!(current_state, BoundaryState::Recording { .. } | BoundaryState::Committing { .. }) {
                    let pcm_bytes = i16_slice_to_u8(chunk);
                    let _ = tx.send(Outgoing::AudioChunk(pcm_bytes.to_vec(), turn_id));
                }
            }
        } else if let Some(ref tx) = self.streaming_tx {
            // Fallback to old streaming system
            if matches!(prev_state, Some(BoundaryState::Idle) | None) && 
               matches!(current_state, BoundaryState::Recording { .. }) {
                let _ = tx.send(TurnInput::StreamingAudio {
                    bytes: vec![],
                    is_start: true,
                    is_end: false,
                });
            }
            
            if matches!(current_state, BoundaryState::Recording { .. } | BoundaryState::Committing { .. }) {
                let pcm_bytes = i16_slice_to_u8(chunk);
                let _ = tx.send(TurnInput::StreamingAudio {
                    bytes: pcm_bytes.to_vec(),
                    is_start: false,
                    is_end: false,
                });
            }
        }
        
        // Update previous state
        self.prev_fsm_state = Some(current_state.clone());
        
        // Process boundary events
        while let Ok(boundary_event) = self.boundary_receiver.try_recv() {
            let seg_id = self.next_asr_id;
            self.next_asr_id += 1;
            
            // Emit end event when segment closes
            if let Some(ref tx) = self.outgoing_tx {
                if let Some(turn_id) = self.current_turn_id {
                    let _ = tx.send(Outgoing::ActivityEnd(turn_id));
                }
                self.current_turn_id = None;
            } else if let Some(ref tx) = self.streaming_tx {
                let _ = tx.send(TurnInput::StreamingAudio {
                    bytes: vec![],
                    is_start: false,
                    is_end: true,
                });
            }
            
            self.emitter.process_boundary_event(boundary_event, seg_id);
        }
        
        // Poll ASR if needed
        if timestamp.duration_since(self.last_asr_poll).as_millis() >= self.config.asr_poll_ms as u128 {
            self.poll_asr();
            self.last_asr_poll = timestamp;
        }
        
        // Return any ready segments
        self.emitter.pop_segment()
    }
    
    fn poll_asr(&mut self) {
        if let Some(seg_range) = self.boundary_fsm.get_current_segment_range() {
            let current_idx = self.ring_buffer.current_global_idx();
            let poll_end = current_idx;
            let poll_start = seg_range.start;
            
            // Check if this is a new segment (segment boundary changed)
            let is_new_segment = self.last_asr_submit_idx
                .map(|last_idx| last_idx < poll_start)
                .unwrap_or(true);
            
            if is_new_segment {
                // Reset tracking for new segment
                self.last_asr_submit_idx = Some(poll_start);
            }
            
            // Only submit new audio that hasn't been processed yet
            let actual_start = self.last_asr_submit_idx.unwrap_or(poll_start);
            
            // Only poll if we have enough NEW audio (at least 0.5 seconds of new data)
            if poll_end > actual_start + 8000 {
                if let Some(audio) = self.ring_buffer.get_range(poll_start..poll_end) {
                    let submitted = self.asr_pool.submit(self.next_asr_id, audio, poll_start..poll_end);
                    if submitted {
                        debug!("Submitted ASR request {} for range {}..{} (full segment)", self.next_asr_id, poll_start, poll_end);
                        // Update tracking to avoid reprocessing
                        self.last_asr_submit_idx = Some(poll_end);
                    }
                }
            }
        } else {
            // No active segment, reset tracking
            self.last_asr_submit_idx = None;
        }
    }

    /// Force close current segment
    pub fn force_close(&mut self) -> Option<SegmentedTurn> {
        // Implementation would force FSM to emit current segment
        self.emitter.pop_segment()
    }
}

/// Convert i16 slice to mutable u8 slice for audio capture
pub fn i16_to_u8_mut(buffer: &mut [i16]) -> &mut [u8] {
    unsafe {
        std::slice::from_raw_parts_mut(
            buffer.as_mut_ptr() as *mut u8,
            buffer.len() * 2,
        )
    }
}

/// Convert i16 slice to u8 slice
pub fn i16_slice_to_u8(slice: &[i16]) -> &[u8] {
    unsafe {
        std::slice::from_raw_parts(
            slice.as_ptr() as *const u8,
            slice.len() * 2,
        )
    }
}

/// Send completed turn to Gemini
pub async fn send_turn_to_gemini(
    turn: &SegmentedTurn,
    gemini_client: &mut crate::gemini_client::GeminiClient,
) -> std::result::Result<(), Box<dyn std::error::Error + Send + Sync>> {
    let pcm_bytes = i16_slice_to_u8(&turn.audio);
    const MAX_WEBSOCKET: usize = 1_000_000; // 1 MiB
    const FIRST_MAX: usize = 256_000; // 0.25 MiB

    // First slice - set activity_start=true
    let first_chunk_size = std::cmp::min(FIRST_MAX, pcm_bytes.len());
    gemini_client
        .send_audio_with_activity(&pcm_bytes[..first_chunk_size], true, false, false)
        .await
        .map_err(|e| -> Box<dyn std::error::Error + Send + Sync> { Box::new(e) })?;

    // Middle slices (if any)
    if pcm_bytes.len() > first_chunk_size {
        for chunk in pcm_bytes[first_chunk_size..].chunks(MAX_WEBSOCKET) {
            gemini_client
                .send_audio_with_activity(chunk, false, false, false)
                .await
                .map_err(|e| -> Box<dyn std::error::Error + Send + Sync> { Box::new(e) })?;
        }
    }

    // Final slice - set activity_end=true
    gemini_client
        .send_audio_with_activity(&[], false, false, true)
        .await
        .map_err(|e| -> Box<dyn std::error::Error + Send + Sync> { Box::new(e) })?;

    debug!("Sent turn {} to Gemini: {} samples, reason: {:?}", 
           turn.id, turn.audio.len(), turn.close_reason);
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::time::Duration;

    #[test]
    fn test_ring_buffer_basic() {
        let ring = AudioRingBuffer::new(1000);
        
        // Push some data
        let samples1 = vec![1, 2, 3, 4, 5];
        let idx1 = ring.push_frame(&samples1);
        assert_eq!(idx1, 0);
        
        let samples2 = vec![6, 7, 8, 9, 10];
        let idx2 = ring.push_frame(&samples2);
        assert_eq!(idx2, 5);
        
        // Get ranges
        let range1 = ring.get_range(0..5).unwrap();
        assert_eq!(range1, samples1);
        
        let range2 = ring.get_range(5..10).unwrap();
        assert_eq!(range2, samples2);
        
        let combined = ring.get_range(0..10).unwrap();
        assert_eq!(combined, [samples1, samples2].concat());
    }

    #[test]
    fn test_ring_buffer_wraparound() {
        let ring = AudioRingBuffer::new(10);
        
        // Fill beyond capacity
        let samples1 = vec![1, 2, 3, 4, 5, 6, 7, 8];
        let idx1 = ring.push_frame(&samples1);
        assert_eq!(idx1, 0);
        
        let samples2 = vec![9, 10, 11, 12, 13];
        let idx2 = ring.push_frame(&samples2);
        assert_eq!(idx2, 8);
        
        // Should be able to get recent data but not old data
        let recent = ring.get_range(8..13).unwrap();
        assert_eq!(recent, samples2);
        
        // Old data should be unavailable
        assert!(ring.get_range(0..5).is_none());
    }
    
    #[test]
    fn test_clause_validation() {
        assert!(is_valid_clause_simple("This is a sentence.", 4));
        assert!(is_valid_clause_simple("Is this a question?", 4));
        assert!(is_valid_clause_simple("This has enough tokens to pass", 4));
        assert!(!is_valid_clause_simple("Too short", 4));
        assert!(is_valid_clause_simple("I think,", 4));
        assert!(is_valid_clause_simple("Going home and", 4));
    }

    #[test]
    fn test_config_defaults() {
        let config = SegConfig::default();
        
        // Test that defaults are reasonable for real-time operation
        assert!(config.open_voiced_frames >= 3); // At least 60ms
        assert!(config.close_silence_ms >= 200); // At least 200ms silence
        assert!(config.max_turn_ms <= 10000);    // No more than 10s turns
        assert!(config.asr_poll_ms <= 500);      // Poll at least every 500ms
        assert!(config.ring_capacity >= 160000); // At least 10s buffer
    }

    #[test]
    fn test_boundary_fsm_state_transitions() {
        let config = SegConfig::default();
        let (_, asr_rx) = std::sync::mpsc::channel();
        let (mut fsm, _boundary_rx) = BoundaryFSM::new(config, asr_rx);
        
        // Should start in Idle state
        assert_eq!(fsm.state, BoundaryState::Idle);
        
        // Create a voiced frame
        let frame = FrameMeta {
            timestamp: Instant::now(),
            start_idx: 0,
            voiced: true,
        };
        
        // Process multiple voiced frames to trigger opening
        for i in 0..10 {
            let mut test_frame = frame.clone();
            test_frame.start_idx = i * 320; // 20ms apart
            fsm.process_frame(&test_frame, (i + 1) * 320);
        }
        
        // Should transition to Recording state
        assert!(matches!(fsm.state, BoundaryState::Recording { .. }));
    }

    #[test]
    fn test_latency_budget_ring_buffer() {
        let ring = AudioRingBuffer::new(320_000); // 20 second buffer
        let frame_size = 1600; // 100ms
        
        let start = Instant::now();
        
        // Simulate 1 second of audio processing
        for i in 0..10 {
            let samples = vec![i as i16; frame_size];
            ring.push_frame(&samples);
            
            // Each push should be very fast
            let elapsed = start.elapsed();
            assert!(elapsed < Duration::from_millis(1)); // < 1ms per push
        }
        
        // Getting ranges should also be fast
        let get_start = Instant::now();
        let _data = ring.get_range(0..16000); // 1 second of audio
        let get_elapsed = get_start.elapsed();
        assert!(get_elapsed < Duration::from_millis(10)); // < 10ms to extract 1s
    }

    #[test]
    fn test_segment_emitter_ordering() {
        let mut config = SegConfig::default();
        config.asr_timeout_ms = 0; // Don't wait for ASR results in test
        let ring = Arc::new(AudioRingBuffer::new(10000));
        let mut emitter = SegmentEmitter::new(config, ring.clone());
        
        // Add some test audio to ring
        let audio1 = vec![1i16; 1600];
        let audio2 = vec![2i16; 1600]; 
        ring.push_frame(&audio1);
        ring.push_frame(&audio2);
        
        // Create out-of-order boundary events
        // Segment 2: from 1600 to 3200 (next 1600 samples)
        emitter.process_boundary_event(
            BoundaryEvent::SilenceClose(1600, 3200),
            2, // segment 2
        );
        // Segment 1: from 0 to 1600 (first 1600 samples)
        emitter.process_boundary_event(
            BoundaryEvent::SilenceClose(0, 1600),
            1, // segment 1  
        );
        
        // Should emit segment 1 first
        let seg1 = emitter.pop_segment();
        assert!(seg1.is_some(), "Expected segment 1 but got None");
        let seg1 = seg1.unwrap();
        assert_eq!(seg1.audio.len(), 1600);
        
        // Then segment 2
        let seg2 = emitter.pop_segment();
        assert!(seg2.is_some());
        let seg2 = seg2.unwrap();
        assert_eq!(seg2.audio.len(), 1600);
        
        // No more segments
        assert!(emitter.pop_segment().is_none());
    }
}

================
File: src/broker.rs
================
use crate::events::{InEvent, TurnInput, WsOut, WsIn, FrameId};
use base64::Engine;
use std::collections::VecDeque;
use std::time::{Duration, Instant};
use tracing::{info, debug, warn};

const VIDEO_GRACE_MS: u64 = 1000;
const MIN_NEW_FRAMES: usize = 2;
const MAX_FRAMES_PER_TURN: usize = 5;

#[derive(Debug)]
pub enum State {
    Idle,
    CollectingSpeech { start: Instant, turn_id: u64 },
    CollectingVideoOnly { start: Instant, turn_id: u64 },
    StreamingSpeech { start: Instant, turn_id: u64 },
}

pub struct Broker {
    state: State,
    recent_frames: Vec<FrameId>,
    last_audio_time: Instant,
    frames_sent_in_turn: usize,
    
    // Latency Tracking
    next_turn_id: u64,
    pending_turns: VecDeque<(u64, Instant)>, // (turn_id, start_time)
    turn_latencies: VecDeque<Duration>,      // Store recent latencies
    max_latencies_to_store: usize,
}

#[derive(Debug)]
pub enum Event {
    Input(InEvent),
    Ws(WsIn),
}

impl Broker {
    pub fn new() -> Self {
        Self {
            state: State::Idle,
            recent_frames: Vec::new(),
            last_audio_time: Instant::now(),
            frames_sent_in_turn: 0,
            next_turn_id: 0,
            pending_turns: VecDeque::new(),
            turn_latencies: VecDeque::new(),
            max_latencies_to_store: 100, // Store last 100 latencies for averaging
        }
    }
    
    fn start_new_tracked_turn(&mut self) -> u64 {
        let turn_id = self.next_turn_id;
        self.next_turn_id += 1;
        let now = Instant::now();
        self.pending_turns.push_back((turn_id, now));
        
        // Console output for turn start
        println!("\n>>> TURN START: ID={} | Pending={} | Time={:?}", 
            turn_id, 
            self.pending_turns.len(),
            now.elapsed()
        );
        
        info!(
            "TURN_TRACKING: Initiated turn_id {}. Pending turns: {}.",
            turn_id,
            self.pending_turns.len()
        );
        turn_id
    }
    
    fn complete_tracked_turn(&mut self) {
        if let Some((turn_id, start_time)) = self.pending_turns.pop_front() {
            let latency = start_time.elapsed();
            self.turn_latencies.push_back(latency);
            if self.turn_latencies.len() > self.max_latencies_to_store {
                self.turn_latencies.pop_front();
            }
            let avg_latency = self.average_latency();
            
            // Detailed console output
            println!("\n========== LATENCY REPORT ==========");
            println!("Turn ID:          {}", turn_id);
            println!("Latency:          {:.2}s ({:.0}ms)", latency.as_secs_f32(), latency.as_millis());
            println!("Pending Turns:    {}", self.pending_turns.len());
            if let Some(avg) = avg_latency {
                println!("Average Latency:  {:.2}s ({:.0}ms)", avg.as_secs_f32(), avg.as_millis());
            }
            println!("====================================\n");
            
            // Also log with tracing
            info!(
                "TURN_TRACKING: Completed turn_id {}. Latency: {:?}. Pending turns: {}. Avg Latency: {:?}.",
                turn_id,
                latency,
                self.pending_turns.len(),
                avg_latency
            );
        } else {
            warn!("TURN_TRACKING: Received GenerationComplete but no pending turns found.");
        }
    }
    
    fn average_latency(&self) -> Option<Duration> {
        if self.turn_latencies.is_empty() {
            None
        } else {
            let sum: Duration = self.turn_latencies.iter().sum();
            Some(sum / self.turn_latencies.len() as u32)
        }
    }
    
    pub fn get_pending_turns_count(&self) -> usize {
        self.pending_turns.len()
    }
    
    pub fn get_average_latency(&self) -> Option<Duration> {
        self.average_latency()
    }
    
    pub fn print_latency_summary(&self) {
        println!("\n########## LATENCY SUMMARY ##########");
        println!("Total Turns Completed: {}", self.next_turn_id);
        println!("Currently Pending:     {}", self.pending_turns.len());
        
        if !self.pending_turns.is_empty() {
            println!("\nPending Turn Details:");
            for (id, start_time) in &self.pending_turns {
                println!("  - Turn {} waiting for {:.2}s", id, start_time.elapsed().as_secs_f32());
            }
        }
        
        if !self.turn_latencies.is_empty() {
            let min = self.turn_latencies.iter().min().unwrap();
            let max = self.turn_latencies.iter().max().unwrap();
            let avg = self.average_latency().unwrap();
            
            println!("\nLatency Statistics (last {} turns):", self.turn_latencies.len());
            println!("  Min:     {:.2}s ({:.0}ms)", min.as_secs_f32(), min.as_millis());
            println!("  Max:     {:.2}s ({:.0}ms)", max.as_secs_f32(), max.as_millis());
            println!("  Average: {:.2}s ({:.0}ms)", avg.as_secs_f32(), avg.as_millis());
        }
        println!("#####################################\n");
    }

    pub fn handle(&mut self, event: Event) -> Vec<WsOut> {
        match event {
            Event::Input(input) => self.handle_input(input),
            Event::Ws(ws) => self.handle_ws(ws),
        }
    }

    fn handle_input(&mut self, input: InEvent) -> Vec<WsOut> {
        match input {
            InEvent::AudioChunk(_chunk) => {
                // Don't update last_audio_time here! 
                // Only completed speech turns should count as "recent audio"
                vec![]
            }
            InEvent::UniqueFrame { jpeg, hash } => {
                self.recent_frames.push(FrameId {
                    jpeg,
                    hash,
                    timestamp: Instant::now(),
                });
                
                if self.recent_frames.len() > 10 {
                    self.recent_frames.remove(0);
                }

                match &self.state {
                    State::Idle => {
                        let frames_since_turn = self.recent_frames.len() - self.frames_sent_in_turn;
                        let time_since_audio = self.last_audio_time.elapsed();
                        
                        debug!("🤔 Video-only turn check: {} frames since last turn, {}ms since audio (need {} frames, {}ms grace)", 
                               frames_since_turn, time_since_audio.as_millis(), MIN_NEW_FRAMES, VIDEO_GRACE_MS);
                        
                        if frames_since_turn >= MIN_NEW_FRAMES 
                            && time_since_audio > Duration::from_millis(VIDEO_GRACE_MS) {
                            info!("✅ Starting video-only turn: {} new frames, {}ms audio silence", 
                                  frames_since_turn, time_since_audio.as_millis());
                            self.start_video_turn()
                        } else {
                            if frames_since_turn < MIN_NEW_FRAMES {
                                debug!("❌ Not enough new frames for video turn ({} < {})", frames_since_turn, MIN_NEW_FRAMES);
                            }
                            if time_since_audio <= Duration::from_millis(VIDEO_GRACE_MS) {
                                debug!("❌ Not enough audio silence for video turn ({}ms <= {}ms)", 
                                       time_since_audio.as_millis(), VIDEO_GRACE_MS);
                            }
                            vec![]
                        }
                    }
                    _ => {
                        debug!("❌ Cannot start video turn - broker not idle (state: {:?})", self.state);
                        vec![]
                    }
                }
            }
        }
    }

    fn handle_ws(&mut self, ws: WsIn) -> Vec<WsOut> {
        match ws {
            WsIn::GenerationComplete => {
                info!("🔄 Broker state transition: {:?} -> Idle (due to GenerationComplete)", self.state);
                self.complete_tracked_turn(); // Process latency for the completed turn
                self.state = State::Idle;
                vec![]
            }
            _ => vec![]
        }
    }

    pub fn handle_speech_turn(&mut self, turn: TurnInput) -> Vec<WsOut> {
        match turn {
            TurnInput::StreamingAudio { bytes, is_start, is_end } => {
                self.handle_streaming_audio(bytes, is_start, is_end)
            }
            TurnInput::SpeechTurn { pcm, t_start, draft_text: _ } => {
                if !matches!(self.state, State::Idle) {
                    info!("❌ Cannot handle speech turn - broker not idle (state: {:?})", self.state);
                    return vec![];
                }
                
                let turn_id = self.start_new_tracked_turn();
                self.last_audio_time = Instant::now();
                info!("🔊 Updated last_audio_time for completed speech turn");
                
                info!("🔄 Broker state transition: Idle -> CollectingSpeech (turn_id: {})", turn_id);
                self.state = State::CollectingSpeech { start: t_start, turn_id };
                let mut messages = vec![];
                
                // Console output for speech turn
                println!("🎤 SPEECH TURN {} | PCM: {} KB | Frames: {}", 
                    turn_id, 
                    pcm.len() / 1024,
                    self.recent_frames.len()
                );
                
                info!("🎯 Starting speech turn {} with {} recent frames", turn_id, self.recent_frames.len());
                
                let start_input = serde_json::json!({ "activityStart": {} });
                info!("📤 Sending activityStart to Gemini for turn_id {}", turn_id);
                messages.push(WsOut::RealtimeInput(start_input));

                // Send audio data
                let audio_size_kb = pcm.len() / 1024;
                let audio_input = serde_json::json!({
                    "audio": {
                        "data": base64::engine::general_purpose::STANDARD.encode(&pcm),
                        "mimeType": "audio/pcm;rate=16000"
                    }
                });
                info!("🎤 Sending audio chunk to Gemini for turn_id {}: {} KB", turn_id, audio_size_kb);
                messages.push(WsOut::RealtimeInput(audio_input));

                let recent_frame_cutoff = Instant::now() - Duration::from_secs(1);
                let frames_to_send: Vec<_> = self.recent_frames.iter()
                    .filter(|f| f.timestamp > recent_frame_cutoff)
                    .take(MAX_FRAMES_PER_TURN)
                    .collect();

                info!("📹 Sending {} video frames to Gemini for turn_id {}", frames_to_send.len(), turn_id);
                for (i, frame) in frames_to_send.iter().enumerate() {
                    let frame_size_kb = frame.jpeg.len() / 1024;
                    let video_input = serde_json::json!({
                        "video": {
                            "data": base64::engine::general_purpose::STANDARD.encode(&frame.jpeg),
                            "mimeType": "image/jpeg"
                        }
                    });
                    debug!("📸 Frame {}: {} KB", i + 1, frame_size_kb);
                    messages.push(WsOut::RealtimeInput(video_input));
                }

                self.frames_sent_in_turn = self.recent_frames.len();

                info!("📤 Sending activityEnd to Gemini for turn_id {}", turn_id);
                messages.push(WsOut::RealtimeInput(serde_json::json!({
                    "activityEnd": {}
                })));

                messages
            }
            _ => vec![]
        }
    }
    
    fn handle_streaming_audio(&mut self, bytes: Vec<u8>, is_start: bool, is_end: bool) -> Vec<WsOut> {
        let mut messages = vec![];
        
        if is_start {
            if !matches!(self.state, State::Idle) {
                info!("❌ Cannot start streaming - broker not idle (state: {:?})", self.state);
                return vec![];
            }
            
            let turn_id = self.start_new_tracked_turn();
            info!("🔄 Broker state transition: Idle -> StreamingSpeech (turn_id: {})", turn_id);
            self.state = State::StreamingSpeech { start: Instant::now(), turn_id };
            self.last_audio_time = Instant::now();
            
            println!("🎙️  STREAMING START | Turn ID: {}", turn_id);
            
            info!("📤 Sending activityStart for streaming audio (turn_id: {})", turn_id);
            messages.push(WsOut::RealtimeInput(serde_json::json!({ "activityStart": {} })));
        }
        
        if !bytes.is_empty() {
            if let State::StreamingSpeech { turn_id, .. } = self.state {
                 let audio_input = serde_json::json!({
                    "audio": {
                        "data": base64::engine::general_purpose::STANDARD.encode(&bytes),
                        "mimeType": "audio/pcm;rate=16000"
                    }
                });
                debug!("🎤 Sending streaming audio chunk for turn_id {}", turn_id);
                messages.push(WsOut::RealtimeInput(audio_input));
            } else {
                debug!("❌ Ignoring audio chunk - not in streaming state or state is inconsistent.");
                return vec![];
            }
        }
        
        if is_end {
            if let State::StreamingSpeech { turn_id, .. } = self.state {
                println!("🎙️  STREAMING END | Turn ID: {}", turn_id);
                info!("📤 Sending activityEnd for streaming audio (turn_id: {})", turn_id);
                messages.push(WsOut::RealtimeInput(serde_json::json!({ "activityEnd": {} })));
                // Note: We don't transition to Idle here or call complete_tracked_turn.
                // That happens when WsIn::GenerationComplete is received.
            } else {
                 info!("❌ Cannot end streaming - not in streaming state (state: {:?}) or state is inconsistent.", self.state);
                 return vec![];
            }
        }
        
        messages
    }

    fn start_video_turn(&mut self) -> Vec<WsOut> {
        let turn_id = self.start_new_tracked_turn();
        info!("🔄 Broker state transition: Idle -> CollectingVideoOnly (turn_id: {})", turn_id);
        self.state = State::CollectingVideoOnly { start: Instant::now(), turn_id };
        let mut messages = vec![];
        
        println!("📹 VIDEO TURN {} | Frames: {}", turn_id, self.recent_frames.len());
        
        info!("🎯 Starting video-only turn {} with {} frames", turn_id, self.recent_frames.len());

        info!("📤 Sending activityStart (video-only) to Gemini for turn_id {}", turn_id);
        messages.push(WsOut::RealtimeInput(serde_json::json!({ "activityStart": {} })));

        let frames_to_send = self.recent_frames.len() - self.frames_sent_in_turn;
        let frames_to_send = frames_to_send.min(MAX_FRAMES_PER_TURN);
        let start_idx = self.recent_frames.len().saturating_sub(frames_to_send);

        // Send video frames
        let frames_slice = &self.recent_frames[start_idx..];
        info!("📹 Sending {} video frames (video-only turn) to Gemini for turn_id {}", frames_slice.len(), turn_id);
        for (i, frame) in frames_slice.iter().enumerate() {
            let frame_size_kb = frame.jpeg.len() / 1024;
            let video_input = serde_json::json!({
                "video": {
                    "data": base64::engine::general_purpose::STANDARD.encode(&frame.jpeg),
                    "mimeType": "image/jpeg"
                }
            });
            debug!("📸 Frame {}: {} KB", i + 1, frame_size_kb);
            messages.push(WsOut::RealtimeInput(video_input));
        }

        self.frames_sent_in_turn = self.recent_frames.len();

        info!("📤 Sending activityEnd (video-only) to Gemini for turn_id {}", turn_id);
        messages.push(WsOut::RealtimeInput(serde_json::json!({
            "activityEnd": {}
        })));

        messages
    }
}

================
File: src/events.rs
================
use smallvec::SmallVec;
use std::time::Instant;

#[derive(Debug, Clone)]
pub enum InEvent {
    AudioChunk(Vec<i16>),
    UniqueFrame { jpeg: Vec<u8>, hash: u64 },
}

/// Messages sent from producers (audio/video) to the websocket writer
#[derive(Debug, Clone)]
pub enum Outgoing {
    ActivityStart(u64),           // turn-id
    AudioChunk(Vec<u8>, u64),     // data, turn-id
    VideoFrame(Vec<u8>, u64),     // jpeg data, turn-id  
    ActivityEnd(u64),             // turn-id
}

#[derive(Debug, Clone)]
pub enum TurnInput {
    SpeechTurn {
        pcm: Vec<u8>,
        t_start: Instant,
        draft_text: Option<String>,
    },
    VideoTurn {
        frames: SmallVec<[FrameId; 8]>,
        t_start: Instant,
    },
    StreamingAudio {
        bytes: Vec<u8>,
        is_start: bool,
        is_end: bool,
    },
}

#[derive(Debug, Clone)]
pub struct FrameId {
    pub jpeg: Vec<u8>,
    pub hash: u64,
    pub timestamp: Instant,
}

#[derive(Debug, Clone)]
pub enum WsOut {
    Setup(serde_json::Value),
    RealtimeInput(serde_json::Value),
    ClientContent(serde_json::Value),
}

#[derive(Debug, Clone)]
pub enum WsIn {
    Text { content: String, is_final: bool },
    GenerationComplete,
    ToolCall { name: String, args: serde_json::Value },
    Error(String),
}

================
File: src/gemini_client.rs
================
//! Redesigned Gemini Live API client with proper WebSocket handling
//!
//! This module implements a WebSocket client for the Gemini Live API using
//! a split sink/stream approach for concurrent reading and writing.

use crate::gemini::{
    ApiResponse, BidiGenerateContentSetup, ClientMessage, Content, GeminiClientConfig, GeminiError,
    GenerationConfig, Part, RealtimeAudio, RealtimeInput, RealtimeVideo, Result, ServerMessage,
    Transcript,
};

use base64::engine::general_purpose;
use base64::Engine; // Add this trait to use encode/decode methods
use futures_util::{SinkExt, StreamExt};
use tokio::sync::{mpsc, Mutex};
use tokio::task::JoinHandle;
use tokio_tungstenite::{connect_async, tungstenite::Message};
use tracing::{debug, error, info};

use std::sync::Arc;
use std::time::Duration;

/// Type alias for the WebSocket split sink, wrapped in Arc<Mutex<>>
type WsSink = Arc<
    Mutex<
        futures_util::stream::SplitSink<
            tokio_tungstenite::WebSocketStream<
                tokio_tungstenite::MaybeTlsStream<tokio::net::TcpStream>,
            >,
            Message,
        >,
    >,
>;

/// Type alias for the WebSocket split stream
type WsStream = futures_util::stream::SplitStream<
    tokio_tungstenite::WebSocketStream<tokio_tungstenite::MaybeTlsStream<tokio::net::TcpStream>>,
>;

/// Connection state of the Gemini client
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
enum ConnectionState {
    Disconnected,
    Connected,
    SetupComplete,
}

/// Redesigned Gemini Live API client with split WebSocket handling
pub struct GeminiClient {
    config: GeminiClientConfig,
    state: ConnectionState,
    session_token: Option<String>,

    // Direct reference to the WebSocket write half for sending messages
    ws_writer: Option<WsSink>,

    // Channel for receiving messages from the WebSocket
    response_rx: mpsc::Receiver<Result<ApiResponse>>,

    // Task handles to keep background tasks alive
    _rx_task: Option<JoinHandle<()>>,
    _tx_task: Option<JoinHandle<()>>,
}

impl GeminiClient {
    /// Create a new Gemini client with the given configuration.
    pub fn new(config: GeminiClientConfig) -> Self {
        // Create dummy channel until connect() is called
        let (_, response_rx) = mpsc::channel(100);

        Self {
            config,
            state: ConnectionState::Disconnected,
            session_token: None,
            ws_writer: None,
            response_rx,
            _rx_task: None,
            _tx_task: None,
        }
    }

    /// Create a new Gemini client from an API key and optional configuration.
    pub fn from_api_key(api_key: &str, config: Option<GeminiClientConfig>) -> Self {
        let mut config = config.unwrap_or_default();
        config.url = format!(
            "wss://generativelanguage.googleapis.com/ws/google.ai.generativelanguage.v1beta.GenerativeService.BidiGenerateContent?key={}",
            api_key
        );
        Self::new(config)
    }

    /// Get a receiver to subscribe to responses without holding the client mutex
    pub fn subscribe(&mut self) -> mpsc::Receiver<Result<ApiResponse>> {
        // Replace self.response_rx with a fresh dummy so we keep ownership
        let (dummy_tx, new_rx) = mpsc::channel(1);
        let old_rx = std::mem::replace(&mut self.response_rx, new_rx);
        // We don't care about dummy_tx – it just satisfies the type system
        let _ = dummy_tx;
        old_rx
    }

    /// Connect to the Live API endpoint and set up the session.
    pub async fn connect_and_setup(&mut self) -> Result<()> {
        self.connect().await?;
        self.setup().await
    }

    /// Connect to the Live API endpoint.
    pub async fn connect(&mut self) -> Result<()> {
        if self.state != ConnectionState::Disconnected {
            return Ok(());
        }

        info!("Connecting to Gemini API at {}", self.config.url);

        // Connect to the WebSocket
        let (ws_stream, resp) = connect_async(&self.config.url)
            .await
            .map_err(GeminiError::WebSocket)?;

        debug!("WebSocket connection response: {:?}", resp);

        // Split the WebSocket into separate sink (write) and stream (read) halves
        let (sink, stream) = ws_stream.split();

        // Wrap the sink in Arc<Mutex<>> to safely share it
        let sink_shared: WsSink = Arc::new(Mutex::new(sink));

        // Store the sink for later use in send()
        self.ws_writer = Some(sink_shared.clone());

        // ------ Set up the inbound message channel ------
        let (response_tx, new_response_rx) = mpsc::channel::<Result<ApiResponse>>(100);

        // Spawn a task to handle inbound messages
        let rx_task = tokio::spawn(async move {
            info!("Inbound message task started");

            // Process incoming messages from the WebSocket
            let mut stream = stream;

            while let Some(message_result) = stream.next().await {
                match message_result {
                    Ok(Message::Text(text)) => {
                        crate::tdbg!("⬅ websocket message received");
                        debug!("Received text message: {}", text);

                        // Parse and handle the server message
                        match serde_json::from_str::<ServerMessage>(&text) {
                            Ok(server_message) => {
                                // Handle the server message based on its type
                                match server_message {
                                    ServerMessage::SetupComplete { .. } => {
                                        if let Err(_) =
                                            response_tx.send(Ok(ApiResponse::SetupComplete)).await
                                        {
                                            error!("Failed to send SetupComplete response");
                                            break;
                                        }
                                    }
                                    ServerMessage::ServerContent { server_content } => {
                                        // Process model content, transcriptions, etc.
                                        if let Err(_) =
                                            handle_server_content(server_content, &response_tx)
                                                .await
                                        {
                                            error!("Failed to handle server content");
                                            break;
                                        }
                                    }
                                    ServerMessage::ToolCall { tool_call } => {
                                        if let Err(_) = response_tx
                                            .send(Ok(ApiResponse::ToolCall(tool_call)))
                                            .await
                                        {
                                            error!("Failed to send ToolCall response");
                                            break;
                                        }
                                    }
                                    ServerMessage::ToolCallCancellation {
                                        tool_call_cancellation,
                                    } => {
                                        let id = tool_call_cancellation["id"]
                                            .as_str()
                                            .unwrap_or("unknown")
                                            .to_string();

                                        if let Err(_) = response_tx
                                            .send(Ok(ApiResponse::ToolCallCancellation(id)))
                                            .await
                                        {
                                            error!("Failed to send ToolCallCancellation response");
                                            break;
                                        }
                                    }
                                    ServerMessage::GoAway { .. } => {
                                        if let Err(_) =
                                            response_tx.send(Ok(ApiResponse::GoAway)).await
                                        {
                                            error!("Failed to send GoAway response");
                                            break;
                                        }
                                    }
                                    ServerMessage::SessionResumptionUpdate {
                                        session_resumption_update,
                                    } => {
                                        let handle = session_resumption_update["newHandle"]
                                            .as_str()
                                            .unwrap_or("")
                                            .to_string();

                                        if let Err(_) = response_tx
                                            .send(Ok(ApiResponse::SessionResumptionUpdate(handle)))
                                            .await
                                        {
                                            error!(
                                                "Failed to send SessionResumptionUpdate response"
                                            );
                                            break;
                                        }
                                    }
                                }
                                crate::tdbg!("✅ websocket message processed");
                            }
                            Err(e) => {
                                error!("Failed to parse server message: {:?}", e);
                                error!("Raw message: {}", text);

                                if let Err(_) =
                                    response_tx.send(Err(GeminiError::Serialization(e))).await
                                {
                                    error!("Failed to send parsing error");
                                    break;
                                }
                                crate::tdbg!("✅ websocket message processing failed");
                            }
                        }
                    }
                    Ok(Message::Binary(bytes)) => {
                        crate::tdbg!("⬅ websocket binary message received");
                        // Try to decode binary message as UTF-8 to see error content
                        if let Ok(text) = String::from_utf8(bytes.to_vec()) {
                            debug!("Received binary message (decoded): {}", text);

                            // Try to parse it as a ServerMessage - binary messages can be valid responses
                            match serde_json::from_str::<ServerMessage>(&text) {
                                Ok(server_message) => {
                                    // Handle the server message based on its type
                                    match server_message {
                                        ServerMessage::SetupComplete { .. } => {
                                            if let Err(_) = response_tx
                                                .send(Ok(ApiResponse::SetupComplete))
                                                .await
                                            {
                                                error!("Failed to send SetupComplete response");
                                                break;
                                            }
                                        }
                                        ServerMessage::ServerContent { server_content } => {
                                            if let Err(_) =
                                                handle_server_content(server_content, &response_tx)
                                                    .await
                                            {
                                                error!("Failed to handle server content");
                                                break;
                                            }
                                        }
                                        ServerMessage::ToolCall { tool_call } => {
                                            if let Err(_) = response_tx
                                                .send(Ok(ApiResponse::ToolCall(tool_call)))
                                                .await
                                            {
                                                error!("Failed to send ToolCall response");
                                                break;
                                            }
                                        }
                                        ServerMessage::ToolCallCancellation {
                                            tool_call_cancellation,
                                        } => {
                                            let id = tool_call_cancellation["id"]
                                                .as_str()
                                                .unwrap_or("unknown")
                                                .to_string();

                                            if let Err(_) = response_tx
                                                .send(Ok(ApiResponse::ToolCallCancellation(id)))
                                                .await
                                            {
                                                error!(
                                                    "Failed to send ToolCallCancellation response"
                                                );
                                                break;
                                            }
                                        }
                                        ServerMessage::GoAway { .. } => {
                                            if let Err(_) =
                                                response_tx.send(Ok(ApiResponse::GoAway)).await
                                            {
                                                error!("Failed to send GoAway response");
                                                break;
                                            }
                                        }
                                        ServerMessage::SessionResumptionUpdate {
                                            session_resumption_update,
                                        } => {
                                            let handle = session_resumption_update["newHandle"]
                                                .as_str()
                                                .unwrap_or("")
                                                .to_string();

                                            if let Err(_) = response_tx
                                                .send(Ok(ApiResponse::SessionResumptionUpdate(
                                                    handle,
                                                )))
                                                .await
                                            {
                                                error!("Failed to send SessionResumptionUpdate response");
                                                break;
                                            }
                                        }
                                    }
                                    crate::tdbg!("✅ websocket binary message processed");
                                }
                                Err(e) => {
                                    error!(
                                        "Failed to parse binary message as server message: {:?}",
                                        e
                                    );
                                    error!("Raw message: {}", text);
                                    crate::tdbg!("✅ websocket binary message processing failed");
                                }
                            }
                        } else {
                            debug!("Received binary message ({} bytes)", bytes.len());
                            crate::tdbg!("✅ websocket binary message skipped (not text)");
                        }
                    }
                    Ok(Message::Close(frame)) => {
                        if let Some(close_frame) = &frame {
                            error!(
                                "WebSocket closed with code {:?} and reason: {}",
                                close_frame.code, close_frame.reason
                            );

                            // Log detailed analysis for common close reasons
                            if close_frame.reason.contains("Invalid") {
                                error!("CRITICAL: Server rejected a request with INVALID_ARGUMENT, check for:");
                                error!(
                                    "1. Mixing audio data with activity flags in the same frame"
                                );
                                error!("2. Using 'activityControl' instead of newer 'automaticActivityDetection'");
                                error!("3. Sending activity signals in automatic detection mode");
                            } else if close_frame.reason.contains("Explicit activity control") {
                                error!(
                                    "CRITICAL: Server rejected explicit activity control markers!"
                                );
                                error!(
                                    "Make sure automaticActivityDetection.disabled is set to true"
                                );
                            }
                        } else {
                            info!("WebSocket closed without details");
                        }

                        // Notify that the connection is closed (for error handling)
                        if let Err(_) = response_tx.send(Err(GeminiError::ConnectionClosed)).await {
                            error!("Failed to send connection closed notification");
                        }

                        // Send a special ApiResponse message to tell main client to clean up writer
                        // This is processed in next_response() and stream_responses() to clear state
                        if let Err(_) = response_tx.send(Ok(ApiResponse::ConnectionClosed)).await {
                            error!("Failed to send connection closed notification for cleanup");
                        }

                        break;
                    }
                    Ok(_) => {
                        // Ignore other message types (ping/pong)
                    }
                    Err(e) => {
                        error!("WebSocket error: {:?}", e);

                        if let Err(_) = response_tx.send(Err(GeminiError::WebSocket(e))).await {
                            error!("Failed to send WebSocket error");
                        }

                        break;
                    }
                }
            }

            info!("Inbound message task terminated");
        });

        // Store the response channel and task handles in the client
        self.response_rx = new_response_rx;
        self._rx_task = Some(rx_task);

        // Update the client state
        self.state = ConnectionState::Connected;
        info!("Connected to Gemini API");

        Ok(())
    }

    /// Initialize a session by sending the setup message.
    pub async fn setup(&mut self) -> Result<()> {
        if self.state == ConnectionState::Disconnected {
            error!("Cannot setup session: Connection is closed");
            return Err(GeminiError::ConnectionClosed);
        }

        if self.state == ConnectionState::SetupComplete {
            info!("Session already set up");
            return Ok(());
        }

        info!("Setting up Gemini session");

        // Create the setup message
        let mut setup = BidiGenerateContentSetup {
            model: self.config.model.clone(),
            // Convert the system instruction to the proper Content format if provided
            system_instruction: self.config.system_instruction.as_ref().map(|instruction| {
                Content {
                    role: Some("SYSTEM".to_string()),
                    parts: vec![Part {
                        text: Some(instruction.clone()),
                    }],
                }
            }),
            ..Default::default()
        };

        // Set up generation config
        let mut generation_config = GenerationConfig {
            response_modalities: vec![self.config.response_modality.as_str().to_string()],
            temperature: self.config.temperature,
            ..Default::default()
        };

        // Add media resolution if specified
        if let Some(resolution) = self.config.media_resolution {
            generation_config.media_resolution = Some(resolution.as_str().to_string());
        }

        setup.generation_config = Some(generation_config);

        // Create realtime_input_config with correct fields for the Live API
        let mut realtime_config = if let Some(token) = &self.session_token {
            serde_json::json!({
                "sessionResumptionConfig": {
                    "handle": token
                }
            })
        } else {
            serde_json::json!({})
        };

        // Configure for client-side VAD (since we're using Whisper-based segmentation)
        let config_map = realtime_config.as_object_mut().unwrap();

        // Disable automatic activity detection since we're doing client-side VAD
        config_map.insert(
            "automaticActivityDetection".to_string(),
            serde_json::json!({
                "disabled": true
            }),
        );

        // Set activity handling for responsive interruptions
        config_map.insert(
            "activityHandling".to_string(),
            serde_json::json!("START_OF_ACTIVITY_INTERRUPTS"),
        );

        // Set turnCoverage to include all input (not just within activity markers)
        config_map.insert(
            "turnCoverage".to_string(),
            serde_json::json!("TURN_INCLUDES_ONLY_ACTIVITY"),
        );

        setup.realtime_input_config = Some(realtime_config);

        info!("Sending setup message with model: {}", setup.model);

        // Send the setup message directly using our send method
        let msg = ClientMessage::Setup { setup };
        if let Err(e) = self.send(&msg).await {
            error!("Failed to send setup message: {:?}", e);
            return Err(e);
        }

        info!("Setup message sent, waiting for acknowledgment");

        // Wait for setup complete response with a timeout
        let setup_completed =
            tokio::time::timeout(Duration::from_secs(10), self.wait_for_setup_complete())
                .await
                .map_err(|_| {
                    error!("Timeout waiting for setup complete message");
                    GeminiError::Timeout
                })??;

        if setup_completed {
            self.state = ConnectionState::SetupComplete;
            info!("Gemini session setup complete");
            Ok(())
        } else {
            error!("Failed to complete Gemini session setup");
            Err(GeminiError::SetupNotComplete)
        }
    }

    /// Wait for the setup complete message.
    async fn wait_for_setup_complete(&mut self) -> Result<bool> {
        let mut attempts = 0;
        while attempts < 10 {
            match self.response_rx.recv().await {
                Some(Ok(ApiResponse::SetupComplete)) => {
                    return Ok(true);
                }
                Some(Ok(_)) => {
                    // Ignore other messages
                    attempts += 1;
                    continue;
                }
                Some(Err(e)) => {
                    // Propagate any errors
                    return Err(e);
                }
                None => {
                    // Channel closed
                    return Err(GeminiError::ChannelClosed);
                }
            }
        }
        Ok(false) // Timed out without seeing SetupComplete
    }

    /// Send a client message to the server using the WebSocket writer.
    pub async fn send(&mut self, msg: &ClientMessage) -> Result<()> {
        // Check if connection is already closed or writer is cleared
        if self.state == ConnectionState::Disconnected || self.ws_writer.is_none() {
            error!("Cannot send message: Connection is closed");
            return Err(GeminiError::ConnectionClosed);
        }

        // Format the JSON based on message type
        let json = match msg {
            ClientMessage::Setup { setup } => {
                // Format the JSON manually to avoid nesting issues
                let setup_json =
                    serde_json::to_string(setup).map_err(GeminiError::Serialization)?;
                // Remove outer braces and wrap in setup: {...}
                let inner = &setup_json[1..setup_json.len() - 1];
                format!("{{\"setup\":{{{}}}}}", inner)
            }
            ClientMessage::ClientContent { client_content } => {
                format!(
                    "{{\"clientContent\":{}}}",
                    serde_json::to_string(client_content).map_err(GeminiError::Serialization)?
                )
            }
            ClientMessage::RealtimeInput { realtime_input } => {
                format!(
                    "{{\"realtimeInput\":{}}}",
                    serde_json::to_string(realtime_input).map_err(GeminiError::Serialization)?
                )
            }
            ClientMessage::ToolResponse { tool_response } => {
                format!(
                    "{{\"toolResponse\":{}}}",
                    serde_json::to_string(tool_response).map_err(GeminiError::Serialization)?
                )
            }
        };

        // Log message type without the full content to avoid spamming the console
        match msg {
            ClientMessage::Setup { .. } => info!("Sending setup message to Gemini API"),
            ClientMessage::ClientContent { .. } => info!("Sending text content to Gemini API"),
            ClientMessage::RealtimeInput { realtime_input } => {
                if realtime_input.audio.is_some() {
                    if realtime_input.audio_stream_end.unwrap_or(false) {
                        info!("📤 Sending end-of-audio signal to Gemini API");
                    } else {
                        info!("📤 Sending audio chunk to Gemini API");
                    }
                } else if realtime_input.video.is_some() {
                    info!("📤 Sending video frame to Gemini API");
                } else if realtime_input.text.is_some() {
                    info!("📤 Sending streaming text to Gemini API");
                } else if realtime_input.activity_start.is_some() {
                    info!("📤 Sending activityStart signal to Gemini API");
                } else if realtime_input.activity_end.is_some() {
                    info!("📤 Sending activityEnd signal to Gemini API");
                }
            }
            ClientMessage::ToolResponse { .. } => info!("Sending tool response to Gemini API"),
        };

        // Use the WebSocket writer directly to send the message
        if let Some(writer) = &self.ws_writer {
            let mut writer_guard = writer.lock().await;
            match writer_guard.send(Message::Text(json.into())).await {
                Ok(_) => {
                    debug!("Message sent successfully");
                    Ok(())
                }
                Err(e) => {
                    error!("Failed to send message: {:?}", e);

                    // If we get a SendAfterClosing error, update our state
                    if e.to_string().contains("SendAfterClosing") {
                        error!("WebSocket is closed - will not try to send more messages");
                        self.state = ConnectionState::Disconnected;
                        // We'll clear the writer after the lock is released
                        drop(writer_guard);
                        self.ws_writer = None; // Prevent future send attempts
                        return Err(GeminiError::WebSocket(e));
                    }

                    Err(GeminiError::WebSocket(e))
                }
            }
        } else {
            error!("WebSocket writer not available (not connected)");
            Err(GeminiError::ConnectionClosed)
        }
    }

    /// Send an audio chunk to the server.
    ///
    /// * `audio_data` - Raw PCM audio bytes
    /// * `activity_start` - Set to true to mark the beginning of speech (user started talking)
    /// * `activity_end` - Set to true to mark the end of speech (user finished talking)
    /// * `is_end` - Set to true to mark the end of the audio stream (used with auto VAD)
    pub async fn send_audio(
        &mut self,
        audio_data: &[u8],
        activity_start: bool,
        activity_end: bool,
        is_end: bool,
    ) -> Result<()> {
        // IMPORTANT: Use the improved send_audio_with_activity method that properly
        // separates audio data and flags into different messages
        self.send_audio_with_activity(audio_data, activity_start, activity_end, is_end)
            .await
    }

    /// Send a video frame to the server.
    pub async fn send_video(&mut self, frame_data: &[u8], mime_type: &str) -> Result<()> {
        // Encode video data as base64
        let data = general_purpose::STANDARD.encode(frame_data);

        let realtime_input = RealtimeInput {
            audio: None,
            video: Some(RealtimeVideo {
                data,
                mime_type: mime_type.to_string(),
            }),
            text: None,
            activity_start: None,
            activity_end: None,
            audio_stream_end: None,
        };

        let msg = ClientMessage::RealtimeInput { realtime_input };
        self.send(&msg).await
    }

    /// Send a text message to the server.
    pub async fn send_text(&mut self, text: &str) -> Result<()> {
        let client_content = serde_json::json!({
            "turns": [{
                "role": "user",
                "parts": [{
                    "text": text
                }]
            }],
            "turnComplete": true
        });

        let msg = ClientMessage::ClientContent { client_content };
        self.send(&msg).await
    }

    /// Send streaming text to the server (e.g. for partial typing).
    pub async fn send_streaming_text(&mut self, text: &str) -> Result<()> {
        let realtime_input = RealtimeInput {
            audio: None,
            video: None,
            text: Some(text.to_string()),
            activity_start: None,
            activity_end: None,
            audio_stream_end: None,
        };

        let msg = ClientMessage::RealtimeInput { realtime_input };
        self.send(&msg).await
    }
    
    /// Send a 20ms audio chunk for streaming (no activity flags)
    pub async fn send_audio_chunk(&mut self, pcm_20ms: &[u8]) -> Result<()> {
        let realtime_input = RealtimeInput {
            audio: Some(RealtimeAudio {
                data: general_purpose::STANDARD.encode(pcm_20ms),
                mime_type: "audio/pcm;rate=16000".to_string(),
            }),
            video: None,
            text: None,
            activity_start: None,
            activity_end: None,
            audio_stream_end: None,
        };
        let msg = ClientMessage::RealtimeInput { realtime_input };
        self.send(&msg).await
    }
    
    /// Send activity start signal (no audio data)
    pub async fn send_activity_start(&mut self) -> Result<()> {
        let realtime_input = RealtimeInput {
            audio: None,
            video: None,
            text: None,
            activity_start: Some(serde_json::json!({})),
            activity_end: None,
            audio_stream_end: None,
        };
        let msg = ClientMessage::RealtimeInput { realtime_input };
        self.send(&msg).await
    }
    
    /// Send activity end signal (no audio data)
    pub async fn send_activity_end(&mut self) -> Result<()> {
        let realtime_input = RealtimeInput {
            audio: None,
            video: None,
            text: None,
            activity_start: None,
            activity_end: Some(serde_json::json!({})),
            audio_stream_end: None,
        };
        let msg = ClientMessage::RealtimeInput { realtime_input };
        self.send(&msg).await
    }

    /// Receive the next response from the server.
    pub async fn next_response(&mut self) -> Option<Result<ApiResponse>> {
        let response = self.response_rx.recv().await;

        // Check if this is the special ConnectionClosed message that requires client-side cleanup
        if let Some(Ok(ApiResponse::ConnectionClosed)) = &response {
            info!("Received ConnectionClosed message, clearing WebSocket writer");
            // Clear the writer to prevent further send attempts
            self.ws_writer = None;
            self.state = ConnectionState::Disconnected;
        }

        response
    }

    /// Simplified helper for sending audio with clean parameter names
    ///
    /// When using with client-side VAD (automaticActivityDetection.disabled = true):
    /// - Set activity_start=true to mark the beginning of a user turn
    /// - Send multiple audio chunks with activity_start/end=false
    /// - Set activity_end=true to mark the end of a user turn
    /// - Set audio_stream_end=true when finished with entire session
    ///
    /// CRITICAL: Never mix audio data with activity flags in the same frame
    /// The API treats all fields in RealtimeInput as a "oneof" union
    pub async fn send_audio_with_activity(
        &mut self,
        audio_data: &[u8],
        activity_start: bool,
        activity_end: bool,
        audio_stream_end: bool,
    ) -> Result<()> {
        // Log what we're doing to help with debugging
        if cfg!(debug_assertions) {
            debug!(
                "send_audio_with_activity: data_len={}, start={}, end={}, stream_end={}",
                audio_data.len(),
                activity_start,
                activity_end,
                audio_stream_end
            );
        }

        // Send activity_start flag in its own frame if requested
        if activity_start {
            let flag_only = RealtimeInput {
                audio: None,
                video: None,
                text: None,
                activity_start: Some(serde_json::json!({})),
                activity_end: None,
                audio_stream_end: None,
            };
            let msg = ClientMessage::RealtimeInput {
                realtime_input: flag_only,
            };
            self.send(&msg).await?;
        }

        // If we have audio data, send it in its own clean frame (NO FLAGS)
        if !audio_data.is_empty() {
            // Send audio data only (no flags) in a clean frame
            let data = general_purpose::STANDARD.encode(audio_data);
            let audio_only = RealtimeInput {
                audio: Some(RealtimeAudio {
                    data,
                    mime_type: "audio/pcm;rate=16000".to_string(),
                }),
                video: None,
                text: None,
                activity_start: None,   // IMPORTANT: No flags with audio data
                activity_end: None,     // IMPORTANT: No flags with audio data
                audio_stream_end: None, // IMPORTANT: No flags with audio data
            };

            let msg = ClientMessage::RealtimeInput {
                realtime_input: audio_only,
            };
            self.send(&msg).await?;
        }

        // Send activity_end or audio_stream_end flags in their own frame if requested
        if activity_end || audio_stream_end {
            let flag_only = RealtimeInput {
                audio: None,
                video: None,
                text: None,
                activity_start: None,
                activity_end: if activity_end {
                    Some(serde_json::json!({}))
                } else {
                    None
                },
                audio_stream_end: if audio_stream_end { Some(true) } else { None },
            };
            let msg = ClientMessage::RealtimeInput {
                realtime_input: flag_only,
            };
            self.send(&msg).await?;
        }

        Ok(())
    }

    /// Stream responses until a condition is met.
    pub async fn stream_responses<F>(&mut self, mut callback: F) -> Result<()>
    where
        F: FnMut(&ApiResponse) -> bool,
    {
        while let Some(response) = self.response_rx.recv().await {
            match &response {
                Ok(ApiResponse::ConnectionClosed) => {
                    info!("Received ConnectionClosed message in stream, clearing WebSocket writer");
                    // Clear the writer to prevent further send attempts
                    self.ws_writer = None;
                    self.state = ConnectionState::Disconnected;

                    // Call the callback with this special message
                    let should_stop = callback(&ApiResponse::ConnectionClosed);
                    if should_stop {
                        break;
                    }
                }
                Ok(resp) => {
                    let should_stop = callback(resp);
                    if should_stop {
                        break;
                    }
                }
                Err(e) => {
                    return Err(e.clone()); // Now this is safe with our manual Clone implementation
                }
            }
        }

        Ok(())
    }

    /// Store a session resumption token for later reconnection.
    pub fn set_session_token(&mut self, token: String) {
        self.session_token = Some(token);
    }

    /// Get the current connection state.
    pub fn state(&self) -> &'static str {
        match self.state {
            ConnectionState::Disconnected => "Disconnected",
            ConnectionState::Connected => "Connected",
            ConnectionState::SetupComplete => "SetupComplete",
        }
    }

    /// Send raw realtime input JSON (for channel-based architecture)
    pub async fn send_realtime_input(&mut self, json: serde_json::Value) -> Result<()> {
        if self.state != ConnectionState::SetupComplete {
            return Err(GeminiError::SetupNotComplete);
        }

        // Parse the JSON value as RealtimeInput
        let realtime_input: RealtimeInput = serde_json::from_value(json)?;
        let message = ClientMessage::RealtimeInput { realtime_input };
        let json_str = serde_json::to_string(&message)?;

        if let Some(ref writer) = self.ws_writer {
            writer
                .lock()
                .await
                .send(Message::Text(json_str.into()))
                .await
                .map_err(|e| GeminiError::WebSocket(e))?;
        } else {
            return Err(GeminiError::ConnectionClosed);
        }

        Ok(())
    }

    /// Send raw client content JSON (for channel-based architecture)
    pub async fn send_client_content(&mut self, json: serde_json::Value) -> Result<()> {
        if self.state != ConnectionState::SetupComplete {
            return Err(GeminiError::SetupNotComplete);
        }

        let message = ClientMessage::ClientContent { client_content: json };
        let json_str = serde_json::to_string(&message)?;

        if let Some(ref writer) = self.ws_writer {
            writer
                .lock()
                .await
                .send(Message::Text(json_str.into()))
                .await
                .map_err(|e| GeminiError::WebSocket(e))?;
        } else {
            return Err(GeminiError::ConnectionClosed);
        }

        Ok(())
    }
}

/// Process server content messages which can contain different types of data.
async fn handle_server_content(
    content: serde_json::Value,
    response_tx: &mpsc::Sender<Result<ApiResponse>>,
) -> Result<()> {
    // Check for input transcription (from audio we sent)
    if let Some(input_transcription) = content.get("inputTranscription") {
        // Safely extract text, providing a default if missing
        let text = match input_transcription.get("text").and_then(|t| t.as_str()) {
            Some(t) => t.to_string(),
            None => {
                tracing::warn!(
                    "Received input transcription without text field: {:?}",
                    input_transcription
                );
                String::new() // Empty string as fallback
            }
        };

        // Safely extract isFinal flag
        let is_final = input_transcription
            .get("isFinal")
            .and_then(|f| f.as_bool())
            .unwrap_or(false);

        // Only send if we have actual text content
        if !text.is_empty() {
            response_tx
                .send(Ok(ApiResponse::InputTranscription(Transcript {
                    text,
                    is_final,
                })))
                .await
                .map_err(|_| {
                    tracing::error!("Failed to send input transcription via channel");
                    GeminiError::ChannelClosed
                })?;
        }
    }

    // Check for output transcription (text of model's speech)
    if let Some(output_transcription) = content.get("outputTranscription") {
        // Safely extract text, providing a default if missing
        let text = match output_transcription.get("text").and_then(|t| t.as_str()) {
            Some(t) => t.to_string(),
            None => {
                tracing::warn!(
                    "Received output transcription without text field: {:?}",
                    output_transcription
                );
                String::new() // Empty string as fallback
            }
        };

        // Safely extract isFinal flag
        let is_final = output_transcription
            .get("isFinal")
            .and_then(|f| f.as_bool())
            .unwrap_or(false);

        // Only send if we have actual text content
        if !text.is_empty() {
            response_tx
                .send(Ok(ApiResponse::OutputTranscription(Transcript {
                    text,
                    is_final,
                })))
                .await
                .map_err(|_| {
                    tracing::error!("Failed to send output transcription via channel");
                    GeminiError::ChannelClosed
                })?;
        }
    }

    // Check for generationComplete flag
    if let Some(generation_complete) = content.get("generationComplete").and_then(|g| g.as_bool()) {
        if generation_complete {
            tracing::info!("Generation complete received from Gemini");
            response_tx
                .send(Ok(ApiResponse::GenerationComplete))
                .await
                .map_err(|_| {
                    tracing::error!("Failed to send GenerationComplete via channel");
                    GeminiError::ChannelClosed
                })?;
        }
    }

    // Check for model turn (the actual response)
    if let Some(model_turn) = content.get("modelTurn") {
        // Get parts array, log warning if missing
        let parts = match model_turn.get("parts").and_then(|p| p.as_array()) {
            Some(parts) => parts,
            None => {
                tracing::warn!("Received model turn without parts array: {:?}", model_turn);
                return Ok(()); // Skip processing if no parts
            }
        };

        // Safely extract completion flag
        let is_complete = content
            .get("generationComplete")
            .and_then(|g| g.as_bool())
            .unwrap_or(false);

        // Process each part in the response
        for part in parts {
            // Check for text response
            if let Some(text) = part.get("text").and_then(|t| t.as_str()) {
                if !text.is_empty() {
                    response_tx
                        .send(Ok(ApiResponse::TextResponse {
                            text: text.to_string(),
                            is_complete,
                        }))
                        .await
                        .map_err(|_| {
                            tracing::error!("Failed to send text response via channel");
                            GeminiError::ChannelClosed
                        })?;
                }
            }
            // Check for audio response (inline data)
            else if let Some(inline_data) = part.get("inlineData") {
                // Try to extract and decode the base64 data
                match inline_data.get("data").and_then(|d| d.as_str()) {
                    Some(data_str) => {
                        match general_purpose::STANDARD.decode(data_str) {
                            Ok(data) => {
                                // Only send if we have actual data
                                if !data.is_empty() {
                                    response_tx
                                        .send(Ok(ApiResponse::AudioResponse { data, is_complete }))
                                        .await
                                        .map_err(|_| {
                                            tracing::error!(
                                                "Failed to send audio response via channel"
                                            );
                                            GeminiError::ChannelClosed
                                        })?;
                                }
                            }
                            Err(e) => {
                                tracing::error!("Failed to decode base64 audio data: {:?}", e);
                                // Continue processing other parts even if one fails
                            }
                        }
                    }
                    None => {
                        tracing::warn!(
                            "Received inline data without data field: {:?}",
                            inline_data
                        );
                    }
                }
            }
        }
    }

    Ok(())
}

================
File: src/gemini_ws_json.rs
================
//! Gemini WebSocket handler that accepts JSON messages directly

use crate::events::WsIn;
use crate::gemini_client::GeminiClient;
use crate::gemini::ApiResponse;
use anyhow::Result;
use tokio::sync::mpsc::{UnboundedReceiver, UnboundedSender};
use tracing::{info, error};

pub async fn run(
    api_key: &str,
    mut rx_json: UnboundedReceiver<serde_json::Value>,
    tx_evt: UnboundedSender<WsIn>,
) -> Result<()> {
    use crate::gemini::GeminiClientConfig;
    
    let mut config = GeminiClientConfig::default();
    config.system_instruction = Some(
        "\
            describe what you see on the screen, if it hasn't changed, respond with '<nothing>' (no quotes). ignore audio. do not repeat yourself.
        \
        ".to_string()
    );
    
    let mut client = GeminiClient::from_api_key(api_key, Some(config));
    
    client.connect().await?;
    client.setup().await?;
    
    let mut response_rx = client.subscribe();
    
    // Handle outgoing JSON messages
    tokio::spawn(async move {
        while let Some(json) = rx_json.recv().await {
            info!("📨 Sending JSON to Gemini: {}", 
                  serde_json::to_string(&json).unwrap_or_default().chars().take(100).collect::<String>());
            if let Err(e) = client.send_realtime_input(json).await {
                error!("❌ Error sending to Gemini: {}", e);
            }
        }
    });
    
    // Handle incoming responses
    while let Some(response) = response_rx.recv().await {
        match response {
            Ok(api_response) => {
                let ws_in = match api_response {
                    ApiResponse::TextResponse { text, is_complete } => {
                        WsIn::Text { content: text, is_final: is_complete }
                    }
                    ApiResponse::GenerationComplete => {
                        WsIn::GenerationComplete
                    }
                    ApiResponse::ToolCall(tool_call) => {
                        WsIn::ToolCall { 
                            name: tool_call.get("name").and_then(|v| v.as_str()).unwrap_or("").to_string(),
                            args: tool_call
                        }
                    }
                    ApiResponse::ConnectionClosed => {
                        error!("Gemini connection closed");
                        break;
                    }
                    _ => continue,
                };
                
                if tx_evt.send(ws_in).is_err() {
                    error!("Failed to send event - channel closed");
                    break;
                }
            }
            Err(e) => {
                error!("Gemini API error: {:?}", e);
                if tx_evt.send(WsIn::Error(format!("{:?}", e))).is_err() {
                    break;
                }
            }
        }
    }
    
    Ok(())
}

================
File: src/gemini_ws.rs
================
use crate::events::{WsOut, WsIn};
use crate::gemini_client::GeminiClient;
use crate::gemini::ApiResponse;
use anyhow::Result;
use tokio::sync::mpsc::{UnboundedReceiver, UnboundedSender};

pub async fn run(
    api_key: &str,
    mut rx_ws: UnboundedReceiver<WsOut>,
    tx_evt: UnboundedSender<WsIn>,
) -> Result<()> {
    use crate::gemini::GeminiClientConfig;
    
    let mut config = GeminiClientConfig::default();
    config.system_instruction = Some(
        "\
            describe what you see on the screen, if it hasn't changed, respond with '<nothing>' (no quotes). ignore audio. do not repeat yourself.
        \
        ".to_string()
    );
    
    let mut client = GeminiClient::from_api_key(api_key, Some(config));
    
    client.connect().await?;
    client.setup().await?;
    
    let mut response_rx = client.subscribe();
    
    tokio::spawn(async move {
        while let Some(msg) = rx_ws.recv().await {
            use tracing::info;
            info!("📨 Received WsOut message for transmission to Gemini");
            if let Err(e) = handle_outgoing(&mut client, msg).await {
                tracing::error!("❌ Error sending to Gemini: {}", e);
            }
        }
    });
    
    while let Some(response) = response_rx.recv().await {
        match response {
            Ok(api_resp) => {
                if let Err(e) = handle_incoming(api_resp, &tx_evt) {
                    eprintln!("Error handling Gemini response: {}", e);
                }
            }
            Err(e) => {
                tx_evt.send(WsIn::Error(e.to_string()))?;
            }
        }
    }
    
    Ok(())
}

async fn handle_outgoing(client: &mut GeminiClient, msg: WsOut) -> Result<()> {
    use tracing::{debug, info};
    
    match msg {
        WsOut::Setup(_json) => {
            debug!("🔧 Handling Setup message (skipped)");
            Ok(())
        }
        WsOut::RealtimeInput(json) => {
            if json.get("video").is_some() {
                info!("📹 Sending video frame to Gemini client");
            } else if json.get("audio").is_some() {
                info!("🎵 Sending audio chunk to Gemini client");
            } else if json.get("activityStart").is_some() {
                info!("🎬 Sending activityStart to Gemini client");
            } else if json.get("activityEnd").is_some() {
                info!("🎬 Sending activityEnd to Gemini client");
            } else {
                info!("📨 Sending other realtime input to Gemini client");
            }
            client.send_realtime_input(json).await?;
            debug!("✅ Realtime input sent successfully");
            Ok(())
        }
        WsOut::ClientContent(json) => {
            info!("💬 Sending client content to Gemini client");
            client.send_client_content(json).await?;
            debug!("✅ Client content sent successfully");
            Ok(())
        }
    }
}

fn handle_incoming(resp: ApiResponse, tx: &UnboundedSender<WsIn>) -> Result<()> {
    use tracing::info;
    
    match resp {
        ApiResponse::TextResponse { text, is_complete } => {
            if is_complete {
                info!("📥 Received complete text response from Gemini: {}", text.chars().take(100).collect::<String>());
            } else {
                info!("📥 Received partial text response from Gemini: {}", text.chars().take(50).collect::<String>());
            }
            tx.send(WsIn::Text {
                content: text,
                is_final: is_complete,
            })?;
            if is_complete {
                info!("✅ Gemini generation complete");
                tx.send(WsIn::GenerationComplete)?;
            }
        }
        ApiResponse::OutputTranscription(transcript) => {
            info!("📥 Received output transcription from Gemini: {}", transcript.text);
            tx.send(WsIn::Text {
                content: transcript.text,
                is_final: transcript.is_final,
            })?;
        }
        ApiResponse::ConnectionClosed | ApiResponse::GoAway => {
            info!("📥 Gemini connection closing");
            tx.send(WsIn::GenerationComplete)?;
        }
        ApiResponse::GenerationComplete => {
            info!("✅ Forwarding GenerationComplete to broker");
            tx.send(WsIn::GenerationComplete)?;
        }
        _ => {}
    }
    
    Ok(())
}

================
File: src/gemini.rs
================
//! Gemini Live API module
//!
//! Provides a client for interacting with Google's Gemini Live API via WebSockets.
//! Handles audio, video, and text streaming with the model.

use serde::{Deserialize, Serialize};
use std::time::Duration;
use tokio_tungstenite::tungstenite::Error as WsError;
use tracing::error;

/// Generation configuration for setup.
#[derive(Debug, Serialize, Deserialize, Default, Clone)]
#[serde(rename_all = "camelCase")]
pub struct GenerationConfig {
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub response_modalities: Vec<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub media_resolution: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub speech_config: Option<serde_json::Value>,
}

/// Content structure for system instructions and messages
#[derive(Debug, Serialize, Deserialize, Default, Clone)]
#[serde(rename_all = "camelCase")]
pub struct Content {
    #[serde(skip_serializing_if = "Option::is_none")]
    pub role: Option<String>, // "SYSTEM" | "USER" | "MODEL"
    pub parts: Vec<Part>,
}

/// Part of a content message
#[derive(Debug, Serialize, Deserialize, Default, Clone)]
pub struct Part {
    #[serde(skip_serializing_if = "Option::is_none")]
    pub text: Option<String>,
}

/// Session setup message.
#[derive(Debug, Serialize, Deserialize, Default, Clone)]
#[serde(rename_all = "camelCase")]
pub struct BidiGenerateContentSetup {
    pub model: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub generation_config: Option<GenerationConfig>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub system_instruction: Option<Content>, // Changed from String to Content
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tools: Option<serde_json::Value>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub realtime_input_config: Option<serde_json::Value>,
}

/// A chunk of realtime input (audio/video/text)
#[derive(Debug, Serialize, Deserialize, Default, Clone)]
#[serde(rename_all = "camelCase")]
pub struct RealtimeInput {
    #[serde(skip_serializing_if = "Option::is_none")]
    pub audio: Option<RealtimeAudio>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub video: Option<RealtimeVideo>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub text: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub activity_start: Option<serde_json::Value>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub activity_end: Option<serde_json::Value>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub audio_stream_end: Option<bool>,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct RealtimeAudio {
    pub data: String,
    #[serde(rename = "mimeType")]
    pub mime_type: String,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct RealtimeVideo {
    pub data: String,
    #[serde(rename = "mimeType")]
    pub mime_type: String,
}

/// Message sent from client to server.
#[derive(Debug, Serialize, Deserialize, Clone)]
#[serde(untagged)]
pub enum ClientMessage {
    Setup { setup: BidiGenerateContentSetup },
    ClientContent { 
        #[serde(rename = "clientContent")]
        client_content: serde_json::Value 
    },
    RealtimeInput { 
        #[serde(rename = "realtimeInput")]
        realtime_input: RealtimeInput 
    },
    ToolResponse { 
        #[serde(rename = "toolResponse")]
        tool_response: serde_json::Value 
    },
}

/// Server -> client messages
#[derive(Debug, Serialize, Deserialize)]
#[serde(untagged)]
pub enum ServerMessage {
    SetupComplete {
        #[serde(rename = "setupComplete")]
        setup_complete: serde_json::Value,
    },
    ServerContent {
        #[serde(rename = "serverContent")]
        server_content: serde_json::Value,
    },
    ToolCall {
        #[serde(rename = "toolCall")]
        tool_call: serde_json::Value,
    },
    ToolCallCancellation {
        #[serde(rename = "toolCallCancellation")]
        tool_call_cancellation: serde_json::Value,
    },
    GoAway {
        #[serde(rename = "goAway")]
        go_away: serde_json::Value,
    },
    SessionResumptionUpdate {
        #[serde(rename = "sessionResumptionUpdate")]
        session_resumption_update: serde_json::Value,
    },
}

/// Error type for Gemini API operations
#[derive(Debug, thiserror::Error)]
pub enum GeminiError {
    #[error("WebSocket error: {0}")]
    WebSocket(#[from] WsError),

    #[error("JSON serialization error: {0}")]
    Serialization(#[from] serde_json::Error),

    #[error("I/O error: {0}")]
    Io(#[from] std::io::Error),

    #[error("Connection closed")]
    ConnectionClosed,

    #[error("Setup not complete")]
    SetupNotComplete,

    #[error("Channel closed")]
    ChannelClosed,

    #[error("Timeout")]
    Timeout,

    #[error("Other error: {0}")]
    Other(String),
}

// Manual Clone implementation that converts non-cloneable errors to strings
impl Clone for GeminiError {
    fn clone(&self) -> Self {
        match self {
            Self::WebSocket(e) => Self::Other(format!("WebSocket error: {}", e)),
            Self::Serialization(e) => Self::Other(format!("Serialization error: {}", e)),
            Self::Io(e) => Self::Other(format!("I/O error: {}", e)),
            Self::ConnectionClosed => Self::ConnectionClosed,
            Self::SetupNotComplete => Self::SetupNotComplete,
            Self::ChannelClosed => Self::ChannelClosed,
            Self::Timeout => Self::Timeout,
            Self::Other(s) => Self::Other(s.clone()),
        }
    }
}

pub type Result<T> = std::result::Result<T, GeminiError>;

/// Transcript from the Gemini API
#[derive(Debug, Clone)]
pub struct Transcript {
    pub text: String,
    pub is_final: bool,
}

/// Response from the Gemini API
#[derive(Debug, Clone)]
pub enum ApiResponse {
    /// Setup has been completed
    SetupComplete,

    /// Transcription of user input
    InputTranscription(Transcript),

    /// Transcription of model output (if using TTS)
    OutputTranscription(Transcript),

    /// Text response from the model
    TextResponse { text: String, is_complete: bool },

    /// Audio response from the model
    AudioResponse { data: Vec<u8>, is_complete: bool },

    /// Model is requesting a tool call
    ToolCall(serde_json::Value),

    /// Model has cancelled a tool call
    ToolCallCancellation(String),

    /// Server will disconnect soon
    GoAway,

    /// Session resumption token provided
    SessionResumptionUpdate(String),

    /// Generation of a response is complete
    GenerationComplete,

    /// Special message indicating connection closed, should trigger client cleanup
    ConnectionClosed,
}

/// Configuration for the Gemini client
#[derive(Debug, Clone)]
pub struct GeminiClientConfig {
    pub url: String,
    pub model: String,
    pub response_modality: ResponseModality,
    pub system_instruction: Option<String>,
    pub temperature: Option<f32>,
    pub media_resolution: Option<MediaResolution>,
    pub reconnect_attempts: usize,
    pub reconnect_delay: Duration,
}

impl Default for GeminiClientConfig {
    fn default() -> Self {
        Self {
            url: String::new(),
            model: "models/gemini-2.0-flash-live-001".to_string(),
            response_modality: ResponseModality::Text,
            system_instruction: None,
            temperature: Some(0.7),
            media_resolution: Some(MediaResolution::Medium),
            reconnect_attempts: 3,
            reconnect_delay: Duration::from_secs(1),
        }
    }
}

/// Response modality options
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ResponseModality {
    Text,
    Audio,
}

impl ResponseModality {
    pub fn as_str(&self) -> &'static str {
        match self {
            Self::Text => "TEXT",
            Self::Audio => "AUDIO",
        }
    }
}

/// Media resolution options for video input
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum MediaResolution {
    Low,
    Medium,
    High,
}

impl MediaResolution {
    pub fn as_str(&self) -> &'static str {
        match self {
            Self::Low => "MEDIA_RESOLUTION_LOW",
            Self::Medium => "MEDIA_RESOLUTION_MEDIUM",
            Self::High => "MEDIA_RESOLUTION_HIGH",
        }
    }
}

================
File: src/main.rs
================
//! main.rs – Event-driven entry point for RhoLive assistant
//!
//! This implements a clean-sheet event-driven architecture that:
//! - Ingests audio continuously, breaks it into clause-size chunks with local VAD/Whisper
//! - Ingests video continuously at ~2 fps, drops duplicates
//! - Decides turn-by-turn whether to speak or stay silent (`<nothing>`)
//! - Drives the Gemini Live WS API correctly with activityStart/activityEnd markers
//!
//! The key improvement is that all turn management is centralized in a single FSM
//! that owns the authoritative "busy" state, eliminating race conditions.

mod events;
mod broker;
mod audio_capture;
mod video_capture;
mod gemini_ws;
mod gemini_ws_json;
mod ws_writer;
pub mod audio_async;
pub mod audio_seg;
mod gemini;
mod gemini_client;
mod screen;
pub mod ui;
mod util;

use crate::events::{InEvent, TurnInput, WsOut, WsIn, Outgoing};
use crate::broker::{Broker, Event};
use audio_seg::{AudioSegmenter, SegConfig, i16_slice_to_u8};
use ui::{launch_ui, AudioSample, ConversationEntry};

use anyhow::Result;
use tokio::sync::mpsc;
use tracing::{debug, error, info};
use std::sync::Arc;
use std::sync::atomic::AtomicU64;
use std::time::Duration;

#[tokio::main(flavor = "multi_thread")]
async fn main() -> Result<()> {
    // Initialize logging with filters
    use tracing_subscriber::{EnvFilter, prelude::*};

    tracing_subscriber::registry()
        .with(
            tracing_subscriber::fmt::layer()
                .with_filter(
                    EnvFilter::new("info")
                        .add_directive("egui_window_glfw_passthrough=warn".parse().unwrap())
                )
        )
        .init();
    std::env::set_var("RUST_BACKTRACE", "full");

    // Get API key
    let api_key = std::env::var("GEMINI_API_KEY")
        .expect("GEMINI_API_KEY environment variable must be set");

    // 1. Channel plumbing
    let (tx_in, mut rx_in) = mpsc::unbounded_channel::<InEvent>();
    let (tx_in_seg, mut rx_in_seg) = mpsc::unbounded_channel::<InEvent>();
    let (tx_ws, rx_ws) = mpsc::unbounded_channel::<WsOut>();
    let (tx_evt, mut rx_evt) = mpsc::unbounded_channel::<WsIn>();

    // Audio segmentation channel (legacy)
    let (seg_tx, mut seg_rx) = mpsc::unbounded_channel::<TurnInput>();
    
    // NEW: Outgoing message channel for all producers
    let (outgoing_tx, outgoing_rx) = mpsc::unbounded_channel::<Outgoing>();
    
    // NEW: Channel for websocket writer to send JSON
    let (ws_json_tx, mut ws_json_rx) = mpsc::unbounded_channel::<serde_json::Value>();
    
    // NEW: Global turn ID generator
    let turn_id_generator = Arc::new(AtomicU64::new(0));

    // UI channels
    let (ui_audio_tx, mut ui_audio_rx) = mpsc::unbounded_channel::<AudioSample>();
    let (ui_conv_tx, mut ui_conv_rx) = mpsc::unbounded_channel::<ConversationEntry>();

    // 2. Launch UI first
    info!("Starting UI...");
    let ui_state = launch_ui();

    // 3. Launch IO tasks
    info!("Starting audio capture...");
    let tx_in_for_audio = tx_in.clone();
    let tx_in_seg_for_audio = tx_in_seg.clone();
    audio_capture::spawn_with_dual_output(tx_in_for_audio, tx_in_seg_for_audio)?;

    info!("Starting video capture...");
    video_capture::spawn_with_outgoing(
        tx_in.clone(),
        outgoing_tx.clone(),
        turn_id_generator.clone()
    )?;

    // 4. Launch audio segmentation task in blocking thread
    let seg_config = SegConfig {
        open_voiced_frames: 4,      // 80ms to open (responsive)
        close_silence_ms: 250,      // 600ms silence to close (reasonable pauses)
        max_turn_ms: 8000,          // 8 seconds max (good for demo)
        min_clause_tokens: 10,       // 4 tokens for clause detection
        asr_poll_ms: 400,           // Poll every 400ms
        ring_capacity: 320_000,     // 20 seconds buffer
        asr_pool_size: 2,           // 2 worker threads
        asr_timeout_ms: 0,       // no timeout
    };
    let (audio_tx, audio_rx) = std::sync::mpsc::channel::<Vec<i16>>();
    let ui_state_seg = ui_state.clone();
    let ui_conv_tx_seg = ui_conv_tx.clone();
    
    // Spawn a task to forward from async to sync channel
    tokio::spawn(async move {
        while let Some(event) = rx_in_seg.recv().await {
            if let InEvent::AudioChunk(chunk) = event {
                if audio_tx.send(chunk).is_err() {
                    break;
                }
            }
        }
    });
    
    // Run segmenter in blocking thread
    let outgoing_tx_clone = outgoing_tx.clone();
    let turn_id_gen_clone = turn_id_generator.clone();
    std::thread::spawn(move || {
        let mut segmenter = AudioSegmenter::new(seg_config, None).unwrap();
        
        // Set up the new outgoing channel
        let (sync_outgoing_tx, sync_outgoing_rx) = std::sync::mpsc::channel();
        segmenter.set_outgoing_sender(sync_outgoing_tx, turn_id_gen_clone);
        
        // Forward outgoing events to the async channel
        std::thread::spawn(move || {
            while let Ok(event) = sync_outgoing_rx.recv() {
                let _ = outgoing_tx_clone.send(event);
            }
        });
        
        // Legacy streaming support (remove later)
        let (stream_tx, stream_rx) = std::sync::mpsc::channel();
        segmenter.set_streaming_sender(stream_tx);
        
        let seg_tx_clone = seg_tx.clone();
        std::thread::spawn(move || {
            while let Ok(event) = stream_rx.recv() {
                let _ = seg_tx_clone.send(event);
            }
        });
        
        while let Ok(chunk) = audio_rx.recv() {
            // Calculate audio level for UI
            let level = chunk.iter().map(|&s| (s as f32).abs()).sum::<f32>() 
                / chunk.len() as f32 / 32768.0;
            
            // Send audio to UI
            let _ = ui_audio_tx.send(AudioSample {
                level,
                timestamp: std::time::Instant::now(),
            });

            // Process through segmenter
            if let Some(turn) = segmenter.push_chunk(&chunk) {
                let pcm_bytes = i16_slice_to_u8(&turn.audio);
                
                // Update UI with user speech
                if let Some(ref text) = turn.text {
                    let entry = ConversationEntry {
                        role: "User".to_string(),
                        text: text.clone(),
                        timestamp: std::time::Instant::now(),
                    };
                    let _ = ui_conv_tx_seg.send(entry.clone());
                    
                    // Also update the UI state directly for immediate display
                    if let Ok(mut state) = ui_state_seg.lock() {
                        state.conversation_history.push_back(entry);
                        while state.conversation_history.len() > 50 {
                            state.conversation_history.pop_front();
                        }
                    }
                }
                
                // Update segments processed counter
                if let Ok(mut state) = ui_state_seg.lock() {
                    state.segments_processed += 1;
                }
                
                seg_tx.send(TurnInput::SpeechTurn {
                    pcm: pcm_bytes.to_vec(),
                    t_start: std::time::Instant::now(),
                    draft_text: turn.text,
                }).unwrap();
            }
        }
    });

    // 5. Launch WebSocket writer task
    let (ws_evt_tx, ws_evt_rx) = mpsc::unbounded_channel::<WsIn>();
    tokio::spawn(async move {
        ws_writer::run_writer(outgoing_rx, ws_json_tx, ws_evt_rx).await;
    });
    
    // 6. Launch Gemini websocket task (using new JSON-based handler)
    let api_key_clone = api_key.clone();
    let tx_evt_for_gemini = tx_evt.clone();
    tokio::spawn(async move {
        if let Err(e) = gemini_ws_json::run(&api_key_clone, ws_json_rx, tx_evt_for_gemini).await {
            error!("Gemini WebSocket error: {}", e);
        }
    });
    
    // We'll forward events to the writer later in the main loop

    // Response handler
    let ui_conv_tx_clone = ui_conv_tx.clone();
    let (evt_broadcast_tx, mut evt_broadcast_rx) = mpsc::unbounded_channel::<WsIn>();
    
    tokio::spawn(async move {
        let mut current_text = String::new();
        let mut turn_start = std::time::Instant::now();
        
        while let Some(event) = evt_broadcast_rx.recv().await {
            match event {
                WsIn::Text { content, is_final } => {
                    if current_text.is_empty() {
                        turn_start = std::time::Instant::now();
                    }
                    
                    current_text.push_str(&content);
                    
                    if !content.trim().is_empty() && content.trim() != "<nothing>" {
                        let _ = ui_conv_tx_clone.send(ConversationEntry {
                            role: "Gemini".to_string(),
                            text: current_text.clone(),
                            timestamp: std::time::Instant::now(),
                        });
                    }
                }
                WsIn::GenerationComplete => {
                    current_text.clear();
                }
                _ => {}
            }
        }
    });

    // 6. Connect UI state updates
    let ui_state_audio = ui_state.clone();
    let ui_state_conv = ui_state.clone();
    
    // Audio level visualization task
    tokio::spawn(async move {
        while let Some(sample) = ui_audio_rx.recv().await {
            if let Ok(mut state) = ui_state_audio.lock() {
                state.audio_samples.push_back(sample);
                // Keep only last 100 samples
                while state.audio_samples.len() > 100 {
                    state.audio_samples.pop_front();
                }
            }
        }
    });
    
    // Conversation update task
    tokio::spawn(async move {
        while let Some(entry) = ui_conv_rx.recv().await {
            if let Ok(mut state) = ui_state_conv.lock() {
                state.conversation_history.push_back(entry);
                // Keep only last 50 entries
                while state.conversation_history.len() > 50 {
                    state.conversation_history.pop_front();
                }
            }
        }
    });

    // 7. Simple event forwarding loop (broker is now just for UI updates)
    info!("Starting event forwarding loop...");
    let ui_state_broker = ui_state.clone();
    
    // Update UI connection status
    if let Ok(mut state) = ui_state_broker.lock() {
        state.connected = true;
        state.status_message = "Connected to Gemini".to_string();
    }
    
    loop {
        tokio::select! {
            // Legacy: Handle old-style speech turns from segmenter
            Some(turn) = seg_rx.recv() => {
                // For now, just log that we received it
                // The real streaming is handled by the Outgoing channel
                match turn {
                    TurnInput::SpeechTurn { pcm, .. } => {
                        info!("Received legacy SpeechTurn ({} KB)", pcm.len() / 1024);
                    }
                    TurnInput::StreamingAudio { .. } => {
                        debug!("Received legacy StreamingAudio event");
                    }
                    _ => {}
                }
            }
            // Handle WebSocket events  
            Some(e) = rx_evt.recv() => {
                match &e {
                    WsIn::GenerationComplete => {
                        info!("✅ Received GenerationComplete");
                        // Forward to writer for latency tracking
                        let _ = ws_evt_tx.send(e.clone());
                    }
                    WsIn::Text { content, .. } => {
                        debug!("📥 Received text response: {}", content.chars().take(50).collect::<String>());
                    }
                    _ => {}
                }
                // Broadcast to UI handler
                let _ = evt_broadcast_tx.send(e);
            }
            // Exit if all channels closed
            else => break,
        }
    }

    Ok(())
}

================
File: src/screen.rs
================
use std::collections::hash_map::DefaultHasher;
use std::error::Error;
use std::fmt;
use std::hash::{Hash, Hasher};
use std::sync::mpsc::Receiver;
use std::time::Duration;
use tracing::{debug, info};
use xcap::{Frame, Monitor, VideoRecorder};

/// Screen capture error that is Send + Sync
#[derive(Debug)]
pub enum ScreenError {
    XcapError(String),
    NoMonitors,
    FrameConversionError(String),
    Other(String),
}

impl fmt::Display for ScreenError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            ScreenError::XcapError(e) => write!(f, "Xcap error: {}", e),
            ScreenError::NoMonitors => write!(f, "No monitors found"),
            ScreenError::FrameConversionError(e) => write!(f, "Frame conversion error: {}", e),
            ScreenError::Other(e) => write!(f, "Screen capture error: {}", e),
        }
    }
}

impl Error for ScreenError {}

// Make it Send + Sync
unsafe impl Send for ScreenError {}
unsafe impl Sync for ScreenError {}

/// Represents a captured screen frame with conversion options.
#[derive(Debug)]
pub struct CapturedFrame {
    /// The raw frame data from XCap
    pub frame: Frame,
    /// The JPEG encoded data, lazily computed
    jpeg_data: Option<Vec<u8>>,
}

impl CapturedFrame {
    /// Create a new CapturedFrame from an XCap Frame
    pub fn new(frame: Frame) -> Self {
        Self {
            frame,
            jpeg_data: None,
        }
    }

    /// Convert the frame to JPEG format for sending to the Gemini API
    pub fn to_jpeg(&mut self) -> Result<&[u8], ScreenError> {
        use tracing::{debug, info};
        
        if self.jpeg_data.is_none() {
            // Convert the raw RGBA buffer to JPEG using turbojpeg
            let width = self.frame.width;
            let height = self.frame.height;
            
            debug!("🔄 Converting {}x{} RGBA frame to JPEG using turbojpeg...", width, height);

            let start = std::time::Instant::now();
            
            // Use turbojpeg for fast JPEG encoding
            let jpeg_buffer = to_jpeg_fast(&self.frame.raw, width, height, 75)
                .map_err(|e| ScreenError::FrameConversionError(format!("TurboJPEG error: {}", e)))?;

            let encoding_time = start.elapsed();
            let jpeg_size_kb = jpeg_buffer.len() / 1024;
            info!("✅ TurboJPEG encoding complete: {} KB in {:.1}ms", jpeg_size_kb, encoding_time.as_secs_f64() * 1000.0);
            self.jpeg_data = Some(jpeg_buffer);
        } else {
            debug!("🔄 Using cached JPEG data");
        }

        let jpeg_data = self.jpeg_data.as_ref().unwrap();
        debug!("📤 Returning JPEG data: {} bytes", jpeg_data.len());
        Ok(jpeg_data)
    }

    /// Returns the MIME type for the encoded image format
    pub fn mime_type(&self) -> &'static str {
        "image/jpeg"
    }

    /// Get the width of the frame
    pub fn width(&self) -> u32 {
        self.frame.width
    }

    /// Get the height of the frame
    pub fn height(&self) -> u32 {
        self.frame.height
    }
    
    /// Calculate a hash of the frame for duplicate detection
    pub fn hash(&self) -> u64 {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};
        
        let mut hasher = DefaultHasher::new();
        
        // Hash a subset of pixels for efficiency
        let step = (self.frame.raw.len() / 1000).max(1);
        for i in (0..self.frame.raw.len()).step_by(step) {
            self.frame.raw[i].hash(&mut hasher);
        }
        
        // Include dimensions in hash
        self.frame.width.hash(&mut hasher);
        self.frame.height.hash(&mut hasher);
        
        hasher.finish()
    }
}

/// Captures frames from the primary monitor using the `xcap` crate.
pub struct ScreenCapturer {
    video_recorder: VideoRecorder,
    frame_rx: Receiver<Frame>,
    capture_interval: Duration,
    last_capture: std::time::Instant,
    monitor_info: MonitorInfo,
    // Frame deduplication tracking
    last_frame_hash: Option<u64>,
}

#[derive(Debug, Clone)]
pub struct MonitorInfo {
    name: String,
    width: u32,
    height: u32,
    is_primary: bool,
}

impl ScreenCapturer {
    /// Create a new screen capturer for the primary monitor with default options.
    pub fn new() -> Result<Self, ScreenError> {
        Self::with_options(Duration::from_millis(500))
    }

    /// Create a new screen capturer for the primary monitor with specified capture interval.
    pub fn with_options(capture_interval: Duration) -> Result<Self, ScreenError> {
        // Get all monitors and use the first one
        let monitors = Monitor::all()
            .map_err(|e| ScreenError::XcapError(e.to_string()))?;
        if monitors.is_empty() {
            return Err(ScreenError::NoMonitors);
        }

        // Find primary monitor if available
        let monitor = monitors
            .iter()
            .find(|m| m.is_primary().unwrap_or(false))
            .unwrap_or(&monitors[0])
            .clone();

        // Store monitor information
        let monitor_info = MonitorInfo {
            name: monitor.name().unwrap_or_else(|_| "Unknown".to_string()),
            width: monitor.width().unwrap_or(0),
            height: monitor.height().unwrap_or(0),
            is_primary: monitor.is_primary().unwrap_or(false),
        };

        info!(
            "Using monitor: {} ({}x{}, primary: {})",
            monitor_info.name, monitor_info.width, monitor_info.height, monitor_info.is_primary
        );

        let (video_recorder, frame_rx) = monitor.video_recorder()
            .map_err(|e| ScreenError::XcapError(e.to_string()))?;
        video_recorder.start()
            .map_err(|e| ScreenError::XcapError(e.to_string()))?;

        Ok(Self {
            video_recorder,
            frame_rx,
            capture_interval,
            last_capture: std::time::Instant::now(),
            monitor_info,
            last_frame_hash: None,
        })
    }

    /// Calculate a hash for a frame to use for deduplication
    fn calculate_frame_hash(frame: &Frame) -> u64 {
        // random number for testing, not a real hash
        return rand::random::<u64>();

        let mut hasher = DefaultHasher::new();

        // Create a smaller sampling of the frame for faster hashing
        // Sample every 20th pixel to get a representative hash
        if !frame.raw.is_empty() {
            let stride = 20 * 4; // Every 20th RGBA pixel
            for i in (0..frame.raw.len()).step_by(stride) {
                if i < frame.raw.len() {
                    frame.raw[i].hash(&mut hasher);
                }
            }
        }

        // Also hash the dimensions
        frame.width.hash(&mut hasher);
        frame.height.hash(&mut hasher);

        // hasher.finish()

    }

    /// Capture a single frame of the screen.
    /// This method respects the configured capture interval.
    pub fn capture_frame(&mut self) -> Result<CapturedFrame, ScreenError> {
        use tracing::{debug, info};
        let now = std::time::Instant::now();

        // Check if we need to throttle frame captures
        if now.duration_since(self.last_capture) < self.capture_interval {
            debug!("🚫 Frame capture throttled: interval not reached");
            return Err(ScreenError::Other("Capture interval not reached".to_string()));
        }
        
        debug!("📸 Starting screen capture...");

        // Try to receive a frame with timeout
        match self.frame_rx.recv_timeout(Duration::from_millis(800)) {
            // Increased timeout
            Ok(frame) => {
                info!("📸 Captured raw frame: {}x{} pixels", frame.width, frame.height);

                // Calculate hash for deduplication
                debug!("🔢 Calculating frame hash for deduplication...");
                let frame_hash = Self::calculate_frame_hash(&frame);

                // Check if it's a duplicate
                if let Some(last_hash) = self.last_frame_hash {
                    if frame_hash == last_hash {
                        debug!("🔄 Duplicate frame detected (hash: {}), skipping", frame_hash);
                        return Err(ScreenError::Other("Duplicate frame".to_string()));
                    } else {
                        debug!("✅ New unique frame detected (hash: {} -> {})", last_hash, frame_hash);
                    }
                } else {
                    debug!("✅ First frame captured (hash: {})", frame_hash);
                }

                // Update state
                self.last_capture = now;
                self.last_frame_hash = Some(frame_hash);

                info!("✅ Screen capture successful, creating CapturedFrame");
                Ok(CapturedFrame::new(frame))
            }
            Err(e) => {
                // Log the error but don't propagate timeout errors as they're expected
                if let std::sync::mpsc::RecvTimeoutError::Timeout = e {
                    debug!("Timed out waiting for screen frame, this is normal");
                    Err(ScreenError::Other("Frame capture timeout".to_string()))
                } else {
                    tracing::error!("Error receiving frame from xcap: {:?}", e);
                    Err(ScreenError::Other(format!("Receive error: {:?}", e)))
                }
            }
        }
    }

    /// Force a frame capture regardless of interval
    pub fn force_capture_frame(&mut self) -> Result<CapturedFrame, ScreenError> {
        // Reset the last capture time
        self.last_capture =
            std::time::Instant::now() - self.capture_interval - Duration::from_millis(1);

        // For forced captures, we'll still capture even if it's a duplicate
        match self.frame_rx.recv_timeout(Duration::from_millis(800)) {
            Ok(frame) => {
                debug!("Forced capture of frame: {}x{}", frame.width, frame.height);

                // Calculate hash for future comparison
                let frame_hash = Self::calculate_frame_hash(&frame);
                self.last_frame_hash = Some(frame_hash);

                // Update state
                self.last_capture = std::time::Instant::now();

                Ok(CapturedFrame::new(frame))
            }
            Err(e) => {
                // Log the error but don't propagate timeout errors as they're expected
                if let std::sync::mpsc::RecvTimeoutError::Timeout = e {
                    debug!("Timed out waiting for forced screen frame");
                    Err(ScreenError::Other("Frame capture timeout".to_string()))
                } else {
                    tracing::error!("Error receiving forced frame from xcap: {:?}", e);
                    Err(ScreenError::Other(format!("Receive error: {:?}", e)))
                }
            }
        }
    }

    /// Configure the capture interval (minimum time between frames)
    pub fn set_capture_interval(&mut self, interval: Duration) {
        self.capture_interval = interval;
    }

    /// Get information about the monitor being captured
    pub fn monitor_info(&self) -> &MonitorInfo {
        &self.monitor_info
    }
}

/// Public function to calculate a quick hash for a frame
pub fn quick_hash(frame: &Frame) -> u64 {
    let mut hasher = DefaultHasher::new();

    // Sample the frame data for faster hashing
    let data = &frame.raw;
    let step = data.len() / 64; // Sample 64 points

    for i in (0..data.len()).step_by(step.max(1)) {
        data[i].hash(&mut hasher);
    }

    // Also hash the dimensions
    frame.width.hash(&mut hasher);
    frame.height.hash(&mut hasher);

    hasher.finish()
}

/// Fast JPEG encoding using libjpeg-turbo
pub fn to_jpeg_fast(rgba: &[u8], width: u32, height: u32, quality: i32) -> turbojpeg::Result<Vec<u8>> {
    use turbojpeg::{compress, Image, PixelFormat, Subsamp};

    // libjpeg-turbo can accept 4-channel input; we just tell it RGBA
    let img = Image {
        pixels: rgba,
        width: width as usize,
        pitch: (width * 4) as usize, // bytes per scanline
        height: height as usize,
        format: PixelFormat::RGBA,
    };

    // Use 4:2:0 subsampling for good compression/quality balance
    let compressed = compress(img, quality, Subsamp::Sub2x2)?;
    Ok(compressed.as_ref().to_vec())
}

================
File: src/ui.rs
================
use egui::{Color32, Context, FontId, RichText, Stroke, Vec2, Pos2, FontFamily, FontDefinitions};
use egui_glow::Painter;
use egui_window_glfw_passthrough::glfw::Context as GlfwContext;
use egui_window_glfw_passthrough::{glfw, GlfwBackend, GlfwConfig};
use std::collections::VecDeque;
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};

/// Audio visualization sample
#[derive(Clone, Debug)]
pub struct AudioSample {
    pub level: f32,
    pub timestamp: Instant,
}

/// Conversation entry
#[derive(Clone, Debug)]
pub struct ConversationEntry {
    pub role: String, // "User" or "Gemini"
    pub text: String,
    pub timestamp: Instant,
}

pub struct UiState {
    /// Whether the audio is currently muted
    pub is_muted: bool,
    /// Current AI response being built
    pub current_ai_response: String,
    /// Conversation history
    pub conversation_history: VecDeque<ConversationEntry>,
    /// Active audio device
    pub audio_device: Option<String>,
    /// List of available audio devices
    pub audio_devices: Vec<(String, String)>, // (name, description)
    /// Current transcript being spoken
    pub current_transcript: String,
    /// Whether user is currently speaking
    pub is_speaking: bool,
    /// Number of segments processed
    pub segments_processed: u32,
    /// Number of frames sent to Gemini
    pub frames_sent: u32,
    /// Audio level samples for visualization
    pub audio_samples: VecDeque<AudioSample>,
    /// Connection status
    pub connected: bool,
    /// Show debug info
    pub show_debug: bool,
    /// Last status message
    pub status_message: String,
    /// UI collapsed state
    pub is_collapsed: bool,
    /// Last activity time for auto-collapse
    pub last_activity: Instant,
    /// Typewriter animation state
    pub typewriter_position: usize,
    pub typewriter_last_update: Instant,
    /// Latency tracking
    pub pending_turns_count: usize,
    pub avg_latency_ms: f32,
}

pub struct UiApp {
    /// Shared state between UI and main application
    state: Arc<Mutex<UiState>>,
    /// Start time for runtime calculation
    start_time: Instant,
    /// Frame counter for FPS
    frame_count: u64,
    last_fps_update: Instant,
    fps: f32,
}

impl UiApp {
    pub fn new() -> Self {
        let mut ui_state = UiState {
            is_muted: false,
            current_ai_response: String::new(),
            conversation_history: VecDeque::with_capacity(100),
            audio_device: None,
            audio_devices: Vec::new(),
            current_transcript: String::new(),
            is_speaking: false,
            segments_processed: 0,
            frames_sent: 0,
            audio_samples: VecDeque::with_capacity(200),
            connected: false,
            show_debug: false,
            status_message: String::from("Ready to assist..."),
            is_collapsed: true, // Start collapsed
            last_activity: Instant::now(),
            typewriter_position: 0,
            typewriter_last_update: Instant::now(),
            pending_turns_count: 0,
            avg_latency_ms: 0.0,
        };
        
        // Initialize with some flat audio samples
        let now = Instant::now();
        for i in 0..100 {
            ui_state.audio_samples.push_back(AudioSample {
                level: 0.0,
                timestamp: now - Duration::from_millis(i * 20),
            });
        }
        
        Self {
            state: Arc::new(Mutex::new(ui_state)),
            start_time: Instant::now(),
            frame_count: 0,
            last_fps_update: Instant::now(),
            fps: 0.0,
        }
    }

    /// Get a clone of the state for use in the main application
    pub fn get_state_handle(&self) -> Arc<Mutex<UiState>> {
        self.state.clone()
    }

    /// Run the UI application
    pub fn run(self) -> Result<(), Box<dyn std::error::Error>> {
        // Fixed dimensions for horizontal bar
        let window_width = 1400; // Wider
        let window_height = 100; // Taller initial height for visibility
        
        // Create a GLFW config that uses transparency
        let config = GlfwConfig {
            window_title: "RhoLive".to_string(),
            size: [window_width as u32, window_height as u32],
            transparent_window: Some(true),
            opengl_window: Some(true),
            glfw_callback: Box::new(|glfw: &mut glfw::Glfw| {
                glfw.window_hint(glfw::WindowHint::Decorated(false));
                glfw.window_hint(glfw::WindowHint::Floating(true));
                glfw.window_hint(glfw::WindowHint::Resizable(false));
                glfw.window_hint(glfw::WindowHint::TransparentFramebuffer(true));
                glfw.window_hint(glfw::WindowHint::AlphaBits(Some(8)));
                glfw.window_hint(glfw::WindowHint::DepthBits(Some(0)));
                glfw.window_hint(glfw::WindowHint::StencilBits(Some(0)));
                // Make sure window can receive focus
                glfw.window_hint(glfw::WindowHint::Focused(true));
                glfw.window_hint(glfw::WindowHint::FocusOnShow(true));
            }),
            window_callback: Box::new(move |window| {
                // Position window at bottom center
                // Default to 1920x1080 if we can't get monitor size
                let window_x = (1920 - window_width) / 2;
                let window_y = 1080 - window_height - 40; // 40px from bottom
                window.set_pos(window_x, window_y);
            }),
        };

        // Create the backend with our config
        let mut backend = GlfwBackend::new(config);
        backend.set_passthrough(false);
        // Enable event polling - CRUCIAL for receiving any events!
        backend.window.set_all_polling(true);
        
        // Make sure window can receive events
        backend.window.show();
        backend.window.focus();
        backend.window.set_mouse_passthrough(false);

        // Set up glow renderer for egui
        let gl = unsafe {
            let gl = egui_glow::glow::Context::from_loader_function(|s| {
                backend.window.get_proc_address(s) as *const _
            });
            Arc::new(gl)
        };

        // Create painter for egui
        let mut painter = Painter::new(gl, "", None, false).expect("Failed to create painter");

        // Create egui context
        let mut ctx = Context::default();

        // Load custom fonts
        configure_fonts(&mut ctx);
        
        // Set up minimal glass-like theme
        configure_style(&mut ctx);
        
        // Increase scroll speed for better user experience
        ctx.options_mut(|o| {
            o.line_scroll_speed = 1200.0; // 3x faster than default (40.0)
        });

        // Get a clone of the shared state
        let state = self.state;
        let _start_time = self.start_time;
        let mut frame_count = self.frame_count;
        let mut last_fps_update = self.last_fps_update;
        let mut fps = self.fps;
        let mut current_height = window_height as f32;
        let target_collapsed_height = 60.0;
        let target_expanded_height = 280.0;

        // Main event loop
        while !backend.window.should_close() {
            // Update FPS counter
            frame_count += 1;
            if last_fps_update.elapsed() >= Duration::from_secs(1) {
                fps = frame_count as f32 / last_fps_update.elapsed().as_secs_f32();
                frame_count = 0;
                last_fps_update = Instant::now();
            }

            // Poll events and get input
            backend.glfw.poll_events();
            backend.tick();
            let raw_input = backend.take_raw_input();

            // Process keyboard shortcuts
            let mut toggle_collapse = false;
            let mut toggle_mute = false;
            
            // First check if window has focus
            if !backend.window.is_focused() {
                // Try to capture focus on mouse click
                if raw_input.events.iter().any(|e| matches!(e, egui::Event::PointerButton { pressed: true, .. })) {
                    backend.window.focus();
                }
            }
            
            for event in &raw_input.events {
                // Debug print key events
                if let egui::Event::Key { key, pressed, modifiers, .. } = event {
                    if *pressed {
                        eprintln!("Key pressed: {:?}, Shift: {}, Ctrl: {}, Cmd: {}", 
                                 key, modifiers.shift, modifiers.ctrl, modifiers.command);
                    }
                }
                
                match event {
                    egui::Event::Key { key: egui::Key::Escape, pressed: true, .. } => {
                        backend.window.set_should_close(true);
                    }
                    egui::Event::Key { key: egui::Key::Space, pressed: true, modifiers, .. } => {
                        if modifiers.shift && modifiers.ctrl {
                            eprintln!("Toggle collapse triggered!");
                            toggle_collapse = true;
                        }
                    }
                    egui::Event::Key { key: egui::Key::M, pressed: true, modifiers, .. } => {
                        if modifiers.shift && modifiers.ctrl {
                            eprintln!("Toggle mute triggered!");
                            toggle_mute = true;
                        }
                    }
                    _ => {}
                }
            }

            // Handle state changes
            if toggle_collapse || toggle_mute {
                let mut state_guard = state.lock().unwrap();
                if toggle_collapse {
                    state_guard.is_collapsed = !state_guard.is_collapsed;
                    state_guard.last_activity = Instant::now();
                }
                if toggle_mute {
                    state_guard.is_muted = !state_guard.is_muted;
                }
            }

            // Check for auto-collapse (30 seconds of inactivity)
            {
                let mut state_guard = state.lock().unwrap();
                if !state_guard.is_collapsed && state_guard.last_activity.elapsed() > Duration::from_secs(30) {
                    state_guard.is_collapsed = true;
                }
            }

            // Animate height changes
            let is_collapsed = state.lock().unwrap().is_collapsed;
            let target_height = if is_collapsed { target_collapsed_height } else { target_expanded_height };
            current_height += (target_height - current_height) * 0.15; // Smooth animation
            
            // Update window size if needed
            if (current_height - target_height).abs() > 0.5 {
                backend.window.set_size(window_width, current_height as i32);
                // Re-position to keep bottom-anchored and horizontally centered
                let window_x = (1920 - window_width) / 2; // Keep centered horizontally
                let window_y = 1080 - (current_height as i32) - 40;
                backend.window.set_pos(window_x, window_y);
            }

            // Clear the framebuffer with transparency
            unsafe {
                use egui_glow::glow::HasContext;
                // Enable blending for transparency
                painter.gl().enable(egui_glow::glow::BLEND);
                painter.gl().blend_func(egui_glow::glow::SRC_ALPHA, egui_glow::glow::ONE_MINUS_SRC_ALPHA);
                // Clear to fully transparent
                painter.gl().clear_color(0.0, 0.0, 0.0, 0.0);
                painter.gl().clear(egui_glow::glow::COLOR_BUFFER_BIT);
            }

            // Begin the UI frame
            let output = ctx.run(raw_input, |ctx| {
                // Request continuous repaint for animations
                ctx.request_repaint();
                
                egui::CentralPanel::default()
                    .frame(
                        egui::Frame::none()
                            .fill(Color32::from_rgba_premultiplied(10, 10, 15, 120)) // Much more transparent
                            .inner_margin(egui::Margin::symmetric(30.0, 8.0))
                            .rounding(8.0),
                    )
                    .show(ctx, |ui| {
                        let mut state_guard = state.lock().unwrap();
                        
                        if state_guard.is_collapsed {
                            // Collapsed view - minimal height
                            // Make the entire area clickable
                            let response = ui.allocate_response(
                                ui.available_size(),
                                egui::Sense::click()
                            );
                            
                            if response.clicked() {
                                state_guard.is_collapsed = false;
                                eprintln!("Clicked to expand!");
                            }
                            
                            // Draw content on top
                            ui.allocate_ui_at_rect(response.rect, |ui| {
                                ui.horizontal(|ui| {
                                    // Status indicator
                                    let (icon, color) = if state_guard.connected {
                                        ("●", Color32::from_rgb(100, 255, 150))
                                    } else {
                                        ("○", Color32::from_rgb(100, 100, 100))
                                    };
                                    ui.label(RichText::new(icon).color(color).size(12.0));
                                    
                                    ui.add_space(10.0);
                                    
                                    // Activity indicator or status
                                    if state_guard.is_speaking {
                                        ui.label(RichText::new("Listening...").size(14.0).color(Color32::from_gray(200)));
                                    } else if !state_guard.current_ai_response.is_empty() {
                                        ui.label(RichText::new("Responding...").size(14.0).color(Color32::from_rgb(150, 220, 255)));
                                    } else {
                                        ui.label(RichText::new(&state_guard.status_message).size(14.0).color(Color32::from_gray(180)));
                                    }
                                    
                                    ui.with_layout(egui::Layout::right_to_left(egui::Align::Center), |ui| {
                                        ui.label(RichText::new("Click to expand").size(11.0).color(Color32::from_gray(120)));
                                    });
                                });
                            });
                        } else {
                            // Expanded view - full horizontal layout
                            ui.vertical(|ui| {
                                // Top bar with status and controls
                                ui.horizontal(|ui| {
                                    // Status dot
                                    let (icon, color) = if state_guard.connected {
                                        ("●", Color32::from_rgb(100, 255, 150))
                                    } else {
                                        ("○", Color32::from_rgb(100, 100, 100))
                                    };
                                    ui.label(RichText::new(icon).color(color).size(14.0));
                                    
                                    ui.add_space(15.0);
                                    
                                    // Current activity or transcript
                                    if state_guard.is_speaking {
                                        let time = ui.ctx().input(|i| i.time) as f32;
                                        let pulse = (time * 3.0).sin() * 0.5 + 0.5;
                                        let color = Color32::from_rgb(
                                            (100.0 + 50.0 * pulse) as u8,
                                            255,
                                            (150.0 + 50.0 * pulse) as u8
                                        );
                                        ui.label(RichText::new("● Listening...").color(color).size(16.0));
                                    } else if !state_guard.current_transcript.is_empty() {
                                        ui.label(RichText::new(&state_guard.current_transcript).size(16.0).color(Color32::from_gray(220)));
                                    } else {
                                        ui.label(RichText::new(&state_guard.status_message).size(16.0).color(Color32::from_gray(180)));
                                    }
                                    
                                    ui.with_layout(egui::Layout::right_to_left(egui::Align::Center), |ui| {
                                        // Mute button
                                        let mute_text = if state_guard.is_muted { "🔇" } else { "🔊" };
                                        if ui.button(RichText::new(mute_text).size(18.0)).clicked() {
                                            state_guard.is_muted = !state_guard.is_muted;
                                        }
                                        
                                        ui.add_space(10.0);
                                        
                                        // Collapse button
                                        if ui.button(RichText::new("—").size(16.0)).clicked() {
                                            state_guard.is_collapsed = true;
                                        }
                                    });
                                });
                                
                                ui.add_space(10.0);
                                
                                // Thin separator line
                                ui.add(egui::Separator::default().spacing(2.0));
                                
                                ui.add_space(10.0);
                                
                                // Main content area with scrolling
                                egui::ScrollArea::vertical()
                                    .max_height(ui.available_height() - 60.0) // Leave room for bottom controls
                                    .auto_shrink([false; 2])
                                    .stick_to_bottom(true) // Auto-scroll to bottom
                                    .scroll_bar_visibility(egui::scroll_area::ScrollBarVisibility::VisibleWhenNeeded)
                                    .animated(true)
                                    .show(ui, |ui| {
                                        if !state_guard.current_ai_response.is_empty() {
                                            // Update typewriter animation
                                            if state_guard.typewriter_last_update.elapsed() > Duration::from_millis(20) {
                                                state_guard.typewriter_position = state_guard.typewriter_position
                                                    .saturating_add(2)
                                                    .min(state_guard.current_ai_response.len());
                                                state_guard.typewriter_last_update = Instant::now();
                                                state_guard.last_activity = Instant::now();
                                            }
                                            
                                            let visible_text = &state_guard.current_ai_response[..state_guard.typewriter_position];
                                            
                                            // Parse and render text with code blocks
                                            render_text_with_code_blocks(ui, visible_text);
                                            
                                            // Show cursor if still typing
                                            if state_guard.typewriter_position < state_guard.current_ai_response.len() {
                                                let time = ui.ctx().input(|i| i.time) as f32;
                                                let cursor_alpha = ((time * 2.0).sin() + 1.0) * 0.5;
                                                ui.label(
                                                    RichText::new("│")
                                                        .size(18.0)
                                                        .color(Color32::from_rgba_premultiplied(
                                                            240, 240, 255, 
                                                            (255.0 * cursor_alpha) as u8
                                                        ))
                                                );
                                            }
                                        } else if state_guard.conversation_history.is_empty() {
                                            // Show placeholder when no content
                                            ui.vertical_centered(|ui| {
                                                ui.add_space(40.0);
                                                ui.label(
                                                    RichText::new("<nothing>")
                                                        .size(18.0)
                                                        .color(Color32::from_gray(100))
                                                        .italics()
                                                );
                                            });
                                        } else {
                                            // Show conversation history
                                            for entry in &state_guard.conversation_history {
                                                ui.group(|ui| {
                                                    ui.horizontal(|ui| {
                                                        if entry.role == "User" {
                                                            ui.label(RichText::new("👤").size(14.0));
                                                        } else {
                                                            ui.label(RichText::new("🤖").size(14.0));
                                                        }
                                                        ui.add_space(8.0);
                                                    });
                                                    render_text_with_code_blocks(ui, &entry.text);
                                                });
                                                ui.add_space(8.0);
                                            }
                                        }
                                    });
                                
                                // Bottom section with audio viz
                                ui.with_layout(egui::Layout::bottom_up(egui::Align::LEFT), |ui| {
                                    ui.horizontal(|ui| {
                                        // Minimal audio visualization
                                        ui.allocate_ui(Vec2::new(300.0, 30.0), |ui| {
                                            draw_horizontal_audio_viz(ui, &state_guard.audio_samples, state_guard.is_speaking);
                                        });
                                        
                                        ui.add_space(20.0);
                                        
                                        // Stats (minimal)
                                        if state_guard.show_debug {
                                            ui.label(
                                                RichText::new(format!(
                                                    "Segments: {} | Frames Sent: {} | FPS: {:.0} | Pending Turns: {} | Avg Latency: {:.0}ms",
                                                    state_guard.segments_processed,
                                                    state_guard.frames_sent,
                                                    fps,
                                                    state_guard.pending_turns_count,
                                                    state_guard.avg_latency_ms
                                                ))
                                                .size(11.0)
                                                .color(Color32::from_gray(120))
                                            );
                                        }
                                    });
                                });
                            });
                        }
                    });
            });

            // Paint the UI using egui_glow painter
            let clipped_primitives = ctx.tessellate(output.shapes, output.pixels_per_point);

            // Get the physical size
            let (fb_width, fb_height) = backend.window.get_framebuffer_size();
            painter.paint_and_update_textures(
                [fb_width as u32, fb_height as u32],
                output.pixels_per_point,
                &clipped_primitives,
                &output.textures_delta,
            );

            // Swap buffers to present the frame
            backend.window.swap_buffers();

            // Sleep to reduce CPU usage
            std::thread::sleep(std::time::Duration::from_millis(10));
        }

        Ok(())
    }
}

/// Render text with code blocks formatted properly
fn render_text_with_code_blocks(ui: &mut egui::Ui, text: &str) {
    let parts: Vec<&str> = text.split("```").collect();
    
    for (i, part) in parts.iter().enumerate() {
        if i % 2 == 0 {
            // Regular text
            if !part.is_empty() {
                ui.label(
                    RichText::new(*part)
                        .size(16.0)
                        .color(Color32::from_rgb(240, 240, 255))
                );
            }
        } else {
            // Code block
            let lines: Vec<&str> = part.lines().collect();
            let lang = lines.first().unwrap_or(&"");
            let code = if lines.len() > 1 {
                lines[1..].join("\n")
            } else {
                part.to_string()
            };
            
            ui.group(|ui| {
                ui.set_width(ui.available_width());
                ui.visuals_mut().extreme_bg_color = Color32::from_rgba_premultiplied(30, 30, 40, 180);
                ui.visuals_mut().override_text_color = Some(Color32::from_rgb(220, 220, 240));
                
                // Language label if present
                if !lang.is_empty() {
                    ui.label(
                        RichText::new(*lang)
                            .size(12.0)
                            .color(Color32::from_rgb(150, 150, 170))
                            .italics()
                    );
                }
                
                // Code content with monospace font
                ui.label(
                    RichText::new(&code)
                        .size(14.0)
                        .font(FontId::new(14.0, FontFamily::Monospace))
                        .color(Color32::from_rgb(220, 220, 240))
                );
            });
        }
    }
}

/// Draw horizontal audio visualization
fn draw_horizontal_audio_viz(ui: &mut egui::Ui, samples: &VecDeque<AudioSample>, is_speaking: bool) {
    let rect = ui.available_rect_before_wrap();
    let painter = ui.painter_at(rect);
    
    // Very subtle background
    painter.rect_filled(
        rect,
        egui::Rounding::same(4.0),
        Color32::from_rgba_premultiplied(30, 30, 40, 50),
    );
    
    if samples.is_empty() {
        return;
    }
    
    // Draw minimal waveform
    let width = rect.width();
    let height = rect.height();
    let center_y = rect.center().y;
    
    let max_samples = 60;
    let samples_to_show: Vec<_> = samples.iter()
        .rev()
        .take(max_samples)
        .rev()
        .collect();
    
    if samples_to_show.len() > 1 {
        let x_step = width / (samples_to_show.len() - 1) as f32;
        
        let color = if is_speaking {
            Color32::from_rgba_premultiplied(100, 255, 150, 150)
        } else {
            Color32::from_rgba_premultiplied(100, 150, 255, 80)
        };
        
        for (i, sample) in samples_to_show.iter().enumerate() {
            let x = rect.left() + i as f32 * x_step;
            let amplitude = sample.level.min(1.0) * height * 0.3;
            
            // Draw vertical line from center
            painter.line_segment(
                [
                    Pos2::new(x, center_y - amplitude),
                    Pos2::new(x, center_y + amplitude)
                ],
                Stroke::new(1.5, color),
            );
        }
    }
    
    ui.allocate_rect(rect, egui::Sense::hover());
}

/// Configure custom fonts
fn configure_fonts(ctx: &mut Context) {
    let mut fonts = FontDefinitions::default();
    
    // Try to load Inter font from assets
    match std::fs::read("assets/Inter-Regular.ttf") {
        Ok(font_data) => {
            fonts.font_data.insert(
                "Inter".to_string(),
                egui::FontData::from_owned(font_data),
            );
            
            // Use Inter as the primary font
            fonts.families.entry(FontFamily::Proportional).or_default().insert(0, "Inter".to_string());
            fonts.families.entry(FontFamily::Monospace).or_default().push("Inter".to_string());
            
            ctx.set_fonts(fonts);
        }
        Err(e) => {
            eprintln!("Failed to load Inter font: {}. Using system defaults.", e);
            // Don't set custom fonts, use defaults
        }
    }
}

/// Configure egui visual style for minimal glass theme
fn configure_style(ctx: &mut Context) {
    let mut style = (*ctx.style()).clone();

    // Set dark theme
    style.visuals.dark_mode = true;

    // Ultra-transparent backgrounds
    style.visuals.panel_fill = Color32::TRANSPARENT;
    style.visuals.window_fill = Color32::TRANSPARENT;
    style.visuals.extreme_bg_color = Color32::TRANSPARENT;
    style.visuals.faint_bg_color = Color32::TRANSPARENT;

    // Text colors - high contrast
    style.visuals.widgets.noninteractive.fg_stroke =
        Stroke::new(1.0, Color32::from_rgb(240, 240, 255));
    style.visuals.widgets.inactive.fg_stroke = Stroke::new(1.0, Color32::from_rgb(220, 220, 240));
    style.visuals.widgets.hovered.fg_stroke = Stroke::new(1.5, Color32::from_rgb(255, 255, 255));
    style.visuals.widgets.active.fg_stroke = Stroke::new(2.0, Color32::from_rgb(255, 255, 255));

    // Minimal button styling
    style.visuals.widgets.inactive.bg_fill = Color32::from_rgba_premultiplied(255, 255, 255, 10);
    style.visuals.widgets.inactive.bg_stroke = Stroke::new(0.5, Color32::from_rgba_premultiplied(255, 255, 255, 30));
    style.visuals.widgets.hovered.bg_fill = Color32::from_rgba_premultiplied(255, 255, 255, 20);
    style.visuals.widgets.active.bg_fill = Color32::from_rgba_premultiplied(255, 255, 255, 30);

    // Subtle rounding
    let mut widgets = style.visuals.widgets.clone();
    widgets.noninteractive.rounding = egui::Rounding::from(4.0);
    widgets.inactive.rounding = egui::Rounding::from(4.0);
    widgets.hovered.rounding = egui::Rounding::from(4.0);
    widgets.active.rounding = egui::Rounding::from(4.0);
    style.visuals.widgets = widgets;

    // Minimal spacing
    style.spacing.item_spacing = Vec2::new(8.0, 6.0);
    style.spacing.window_margin = egui::Margin::same(0.0);
    style.spacing.button_padding = Vec2::new(8.0, 4.0);
    
    // Increase scroll sensitivity
    style.spacing.scroll.bar_width = 10.0;
    style.spacing.scroll.handle_min_length = 20.0;
    style.spacing.scroll.bar_inner_margin = 2.0;
    style.spacing.scroll.bar_outer_margin = 2.0;
    style.spacing.scroll.floating = true;

    ctx.set_style(style);
}

/// Launch the UI in a separate thread
pub fn launch_ui() -> Arc<Mutex<UiState>> {
    let app = UiApp::new();
    let state_handle = app.get_state_handle();

    // Launch UI in a separate thread
    std::thread::spawn(move || {
        if let Err(error) = app.run() {
            eprintln!("UI error: {}", error);
        }
    });

    // Give UI time to initialize
    std::thread::sleep(Duration::from_millis(100));

    state_handle
}

================
File: src/util.rs
================
//! Utility functions and macros for debugging

/// Debugging macro that prints the current time (ms since program start) and the thread name or ID,
/// along with the provided message. Integrates with the tracing framework.
#[macro_export]
macro_rules! tdbg {
    ($($arg:tt)*) => {{
        use std::time::{SystemTime, UNIX_EPOCH};
        let ms = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_millis();
        tracing::debug!("[{:>11} ms][{:?}] {}", ms, std::thread::current().name().unwrap_or("unnamed"), format_args!($($arg)*));
    }};
}

// The macro is exported at crate root already, no need for re-export

================
File: src/video_capture.rs
================
use crate::events::{InEvent, Outgoing};
use crate::screen::{ScreenCapturer, quick_hash};
use anyhow::Result;
use tokio::sync::mpsc::UnboundedSender;
use tokio::time::{interval, Duration};
use tracing::{debug, info, error};
use std::sync::Arc;
use std::sync::atomic::{AtomicU64, Ordering};

const FPS: u64 = 2;

pub fn spawn(tx: UnboundedSender<InEvent>) -> Result<()> {
    info!("🎬 Starting video capture task at {} FPS", FPS);
    tokio::spawn(async move {
        if let Err(e) = capture_loop(tx).await {
            error!("Video capture error: {}", e);
        }
    });
    Ok(())
}

pub fn spawn_with_outgoing(
    tx: UnboundedSender<InEvent>, 
    outgoing_tx: UnboundedSender<Outgoing>,
    turn_id_gen: Arc<AtomicU64>,
) -> Result<()> {
    info!("🎬 Starting video capture task at {} FPS (with outgoing channel)", FPS);
    tokio::spawn(async move {
        if let Err(e) = capture_loop_with_outgoing(tx, outgoing_tx, turn_id_gen).await {
            error!("Video capture error: {}", e);
        }
    });
    Ok(())
}

async fn capture_loop(tx: UnboundedSender<InEvent>) -> Result<()> {
    info!("🎬 Initializing video capture loop...");
    let mut capturer = ScreenCapturer::new()?;
    let mut ticker = interval(Duration::from_millis(1000 / FPS));
    let mut last_hash = 0u64;
    info!("🎬 Video capture loop started, waiting for frames...");

    loop {
        ticker.tick().await;
        debug!("⏰ Video capture tick - attempting frame capture...");


        match capturer.capture_frame() {
            Ok(mut frame) => {
                debug!("📸 Frame captured successfully, calculating hash...");
                let hash = quick_hash(&frame.frame);
                
                if hash != last_hash {
                    info!("🆕 New unique frame detected (hash: {} -> {})", last_hash, hash);
                    last_hash = hash;
                    
                    match frame.to_jpeg() {
                        Ok(jpeg_data) => {
                            let jpeg = jpeg_data.to_vec();
                            let jpeg_size_kb = jpeg.len() / 1024;
                            info!("📤 Sending UniqueFrame event: {} KB JPEG (hash: {})", jpeg_size_kb, hash);
                            if tx.send(InEvent::UniqueFrame { jpeg, hash }).is_err() {
                                error!("❌ Failed to send frame event - channel closed");
                                break;
                            }
                            debug!("✅ Frame event sent successfully");
                        }
                        Err(e) => {
                            error!("❌ JPEG conversion error: {}", e);
                            continue;
                        }
                    }
                } else {
                    debug!("🔄 Duplicate frame skipped (hash: {})", hash);
                }
            }
            Err(e) => {
                debug!("❌ Frame capture error: {}", e);
                continue;
            }
        }
    }

    Ok(())
}

async fn capture_loop_with_outgoing(
    tx: UnboundedSender<InEvent>,
    outgoing_tx: UnboundedSender<Outgoing>,
    turn_id_gen: Arc<AtomicU64>,
) -> Result<()> {
    info!("🎬 Initializing video capture loop with outgoing channel...");
    let mut capturer = ScreenCapturer::new()?;
    let mut ticker = interval(Duration::from_millis(1000 / FPS));
    let mut last_hash = 0u64;
    let mut current_turn_id: Option<u64> = None;
    info!("🎬 Video capture loop started, waiting for frames...");

    loop {
        ticker.tick().await;
        debug!("⏰ Video capture tick - attempting frame capture...");

        match capturer.capture_frame() {
            Ok(mut frame) => {
                debug!("📸 Frame captured successfully, calculating hash...");
                let hash = quick_hash(&frame.frame);
                
                if hash != last_hash {
                    info!("🆕 New unique frame detected (hash: {} -> {})", last_hash, hash);
                    last_hash = hash;
                    
                    match frame.to_jpeg() {
                        Ok(jpeg_data) => {
                            let jpeg = jpeg_data.to_vec();
                            let jpeg_size_kb = jpeg.len() / 1024;
                            
                            // Get or create turn ID for this frame
                            let turn_id = current_turn_id.unwrap_or_else(|| {
                                let id = turn_id_gen.load(Ordering::SeqCst).saturating_sub(1);
                                if id == 0 {
                                    // No active turn yet, frames will be queued
                                    0
                                } else {
                                    id
                                }
                            });
                            
                            // Send via new outgoing channel
                            info!("📤 Sending video frame for turn {}: {} KB JPEG (hash: {})", 
                                  turn_id, jpeg_size_kb, hash);
                            if outgoing_tx.send(Outgoing::VideoFrame(jpeg.clone(), turn_id)).is_err() {
                                error!("❌ Failed to send frame via outgoing channel - channel closed");
                                break;
                            }
                            
                            // Also send legacy event
                            if tx.send(InEvent::UniqueFrame { jpeg, hash }).is_err() {
                                error!("❌ Failed to send frame event - channel closed");
                                break;
                            }
                            debug!("✅ Frame sent successfully");
                        }
                        Err(e) => {
                            error!("❌ JPEG conversion error: {}", e);
                            continue;
                        }
                    }
                } else {
                    debug!("🔄 Duplicate frame skipped (hash: {})", hash);
                }
            }
            Err(e) => {
                debug!("❌ Frame capture error: {}", e);
                continue;
            }
        }
    }

    Ok(())
}

================
File: src/ws_writer.rs
================
//! WebSocket writer task that serializes and sends all outgoing messages
//! This is the single point where all producers' messages are serialized to JSON

use crate::events::{Outgoing, WsIn};
use base64::Engine;
use serde_json::json;
use tokio::sync::mpsc::{UnboundedReceiver, UnboundedSender};
use tracing::{debug, info, error};
use std::collections::VecDeque;
use std::time::Instant;

/// Latency tracking for turns
struct TurnTracker {
    pending_turns: VecDeque<(u64, Instant)>,
    completed_turns: VecDeque<(u64, std::time::Duration)>,
    max_completed: usize,
}

impl TurnTracker {
    fn new() -> Self {
        Self {
            pending_turns: VecDeque::new(),
            completed_turns: VecDeque::new(),
            max_completed: 100,
        }
    }
    
    fn start_turn(&mut self, turn_id: u64) {
        self.pending_turns.push_back((turn_id, Instant::now()));
        println!("\n>>> TURN START: ID={} | Pending={}", turn_id, self.pending_turns.len());
    }
    
    fn complete_turn(&mut self) {
        if let Some((turn_id, start_time)) = self.pending_turns.pop_front() {
            let latency = start_time.elapsed();
            self.completed_turns.push_back((turn_id, latency));
            
            if self.completed_turns.len() > self.max_completed {
                self.completed_turns.pop_front();
            }
            
            // Print latency report
            println!("\n========== LATENCY REPORT ==========");
            println!("Turn ID:          {}", turn_id);
            println!("Latency:          {:.2}s ({:.0}ms)", latency.as_secs_f32(), latency.as_millis());
            println!("Pending Turns:    {}", self.pending_turns.len());
            
            if !self.completed_turns.is_empty() {
                let sum: std::time::Duration = self.completed_turns.iter().map(|(_, d)| *d).sum();
                let avg = sum / self.completed_turns.len() as u32;
                println!("Average Latency:  {:.2}s ({:.0}ms)", avg.as_secs_f32(), avg.as_millis());
            }
            println!("====================================\n");
        }
    }
    
    fn print_summary(&self) {
        println!("\n########## LATENCY SUMMARY ##########");
        println!("Currently Pending:     {}", self.pending_turns.len());
        
        if !self.pending_turns.is_empty() {
            println!("\nPending Turn Details:");
            for (id, start_time) in &self.pending_turns {
                println!("  - Turn {} waiting for {:.2}s", id, start_time.elapsed().as_secs_f32());
            }
        }
        
        if !self.completed_turns.is_empty() {
            let latencies: Vec<_> = self.completed_turns.iter().map(|(_, d)| *d).collect();
            let min = latencies.iter().min().unwrap();
            let max = latencies.iter().max().unwrap();
            let sum: std::time::Duration = latencies.iter().sum();
            let avg = sum / latencies.len() as u32;
            
            println!("\nLatency Statistics (last {} turns):", latencies.len());
            println!("  Min:     {:.2}s ({:.0}ms)", min.as_secs_f32(), min.as_millis());
            println!("  Max:     {:.2}s ({:.0}ms)", max.as_secs_f32(), max.as_millis());
            println!("  Average: {:.2}s ({:.0}ms)", avg.as_secs_f32(), avg.as_millis());
        }
        println!("#####################################\n");
    }
}

/// Run the websocket writer task
pub async fn run_writer(
    mut outgoing_rx: UnboundedReceiver<Outgoing>,
    websocket_tx: UnboundedSender<serde_json::Value>,
    mut ws_event_rx: UnboundedReceiver<WsIn>,
) {
    info!("WebSocket writer task started");
    
    let mut tracker = TurnTracker::new();
    let mut last_summary = Instant::now();
    let summary_interval = std::time::Duration::from_secs(30);
    
    loop {
        tokio::select! {
            // Handle outgoing messages from producers
            Some(msg) = outgoing_rx.recv() => {
                let json = match msg {
                    Outgoing::ActivityStart(turn_id) => {
                        tracker.start_turn(turn_id);
                        info!("📤 Sending activityStart for turn {}", turn_id);
                        json!({"activityStart": {}})
                    }
                    Outgoing::AudioChunk(bytes, turn_id) => {
                        debug!("🎤 Sending audio chunk for turn {} ({} bytes)", turn_id, bytes.len());
                        json!({
                            "audio": {
                                "data": base64::engine::general_purpose::STANDARD.encode(&bytes),
                                "mimeType": "audio/pcm;rate=16000"
                            }
                        })
                    }
                    Outgoing::VideoFrame(jpeg, turn_id) => {
                        debug!("📹 Sending video frame for turn {} ({} KB)", turn_id, jpeg.len() / 1024);
                        json!({
                            "video": {
                                "data": base64::engine::general_purpose::STANDARD.encode(&jpeg),
                                "mimeType": "image/jpeg"
                            }
                        })
                    }
                    Outgoing::ActivityEnd(turn_id) => {
                        info!("📤 Sending activityEnd for turn {}", turn_id);
                        json!({"activityEnd": {}})
                    }
                };
                
                if let Err(e) = websocket_tx.send(json) {
                    error!("Failed to send to websocket: {}", e);
                    break;
                }
            }
            
            // Handle incoming WS events for latency tracking
            Some(event) = ws_event_rx.recv() => {
                match event {
                    WsIn::GenerationComplete => {
                        tracker.complete_turn();
                    }
                    _ => {}
                }
            }
            
            // All channels closed
            else => {
                info!("WebSocket writer task shutting down");
                break;
            }
        }
        
        // Print periodic summary
        if last_summary.elapsed() >= summary_interval {
            tracker.print_summary();
            last_summary = Instant::now();
        }
    }
}



================================================================
End of Codebase
================================================================
